{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install pandas\n",
    "%pip install numpy\n",
    "%pip install tqdm\n",
    "%pip install scikit-learn\n",
    "%pip install scipy\n",
    "%pip install warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T06:32:38.542141Z",
     "iopub.status.busy": "2025-05-27T06:32:38.541580Z",
     "iopub.status.idle": "2025-05-27T06:32:42.577743Z",
     "shell.execute_reply": "2025-05-27T06:32:42.576694Z",
     "shell.execute_reply.started": "2025-05-27T06:32:38.542109Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from __future__ import print_function\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy import sparse \n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn import linear_model\n",
    "import random\n",
    "\n",
    "import multiprocessing as mp\n",
    "from functools import partial\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T06:32:42.579952Z",
     "iopub.status.busy": "2025-05-27T06:32:42.579428Z",
     "iopub.status.idle": "2025-05-27T06:32:42.590183Z",
     "shell.execute_reply": "2025-05-27T06:32:42.588938Z",
     "shell.execute_reply.started": "2025-05-27T06:32:42.579921Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def split_no_cold_start(ratings_df, test_size=0.2, min_ratings_per_user=5, min_ratings_per_item=5):\n",
    "    df = ratings_df.copy()\n",
    "    \n",
    "    user_counts = df['user_id'].value_counts()\n",
    "    item_counts = df['item_id'].value_counts()\n",
    "    \n",
    "    valid_users = user_counts[user_counts >= min_ratings_per_user].index\n",
    "    valid_items = item_counts[item_counts >= min_ratings_per_item].index\n",
    "    \n",
    "    df_filtered = df[df['user_id'].isin(valid_users) & df['item_id'].isin(valid_items)]\n",
    "    \n",
    "    train_data = []\n",
    "    test_data = []\n",
    "    \n",
    "    # For each user, ensure they appear in both train and test\n",
    "    for user_id in tqdm(df_filtered['user_id'].unique(), desc=\"Processing users\"):\n",
    "        user_ratings = df_filtered[df_filtered['user_id'] == user_id]\n",
    "        \n",
    "        if len(user_ratings) >= 2:\n",
    "            # Split user's ratings\n",
    "            user_train, user_test = train_test_split(\n",
    "                user_ratings, \n",
    "                test_size=test_size, \n",
    "                random_state=42\n",
    "            )\n",
    "            train_data.append(user_train)\n",
    "            test_data.append(user_test)\n",
    "    \n",
    "    train_df = pd.concat(train_data, ignore_index=True)\n",
    "    test_df = pd.concat(test_data, ignore_index=True)\n",
    "    \n",
    "    return train_df, test_df\n",
    "\n",
    "def calculate_rmse(model, test_data):\n",
    "    n_tests = test_data.shape[0]\n",
    "    SE = 0\n",
    "    \n",
    "    for n in tqdm(range(n_tests), desc=\"Calculating RMSE\"):\n",
    "        pred = model.pred(test_data[n, 0], test_data[n, 1], normalized=0)\n",
    "        SE += (pred - test_data[n, 2])**2 \n",
    "\n",
    "    return np.sqrt(SE/n_tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T06:32:42.592024Z",
     "iopub.status.busy": "2025-05-27T06:32:42.591560Z",
     "iopub.status.idle": "2025-05-27T06:32:42.967888Z",
     "shell.execute_reply": "2025-05-27T06:32:42.966764Z",
     "shell.execute_reply.started": "2025-05-27T06:32:42.591987Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ratings = pd.read_csv(\"/kaggle/input/movie-lens-1m/ratings.csv\", encoding='latin-1')\n",
    "ratings = pd.read_csv(\"/kaggle/input/movie-lens-1m/amazon_ratings.csv\", encoding=\"latin-1\")\n",
    "\n",
    "print(f\"Dataset shape: {ratings.shape}\")\n",
    "print(f\"Unique users: {ratings['user_id'].nunique()}\")\n",
    "print(f\"Unique movies: {ratings['item_id'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T06:32:42.969966Z",
     "iopub.status.busy": "2025-05-27T06:32:42.969626Z",
     "iopub.status.idle": "2025-05-27T06:34:51.142718Z",
     "shell.execute_reply": "2025-05-27T06:34:51.141096Z",
     "shell.execute_reply.started": "2025-05-27T06:32:42.969941Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "rate_train, rate_test = split_no_cold_start(ratings, test_size=0.1)\n",
    "\n",
    "print(f\"Train set shape: {rate_train.shape}\")\n",
    "print(f\"Test set shape: {rate_test.shape}\")\n",
    "print(f\"Train users: {rate_train['user_id'].nunique()}\")\n",
    "print(f\"Test users: {rate_test['user_id'].nunique()}\")\n",
    "print(f\"Train movies: {rate_train['item_id'].nunique()}\")\n",
    "print(f\"Test movies: {rate_test['item_id'].nunique()}\")\n",
    "\n",
    "# Checking data integrity\n",
    "assert rate_train['user_id'].nunique() == rate_test['user_id'].nunique(), \"Different number of users in train/test\"\n",
    "assert len(set(rate_train['user_id'].unique()) - set(rate_test['user_id'].unique())) == 0, \"Cold start users detected\"\n",
    "assert len(set(rate_test['item_id'].unique()) - set(rate_train['item_id'].unique())) == 0, \"Cold start movies detected\"\n",
    "assert rate_train.shape[0] + rate_test.shape[0] <= ratings.shape[0], \"Data leakage detected\"\n",
    "assert rate_train['rating'].isnull().sum() == 0, \"Null ratings in train set\"\n",
    "assert rate_test['rating'].isnull().sum() == 0, \"Null ratings in test set\"\n",
    "\n",
    "print(\"All integrity checks passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T06:34:51.144523Z",
     "iopub.status.busy": "2025-05-27T06:34:51.144264Z",
     "iopub.status.idle": "2025-05-27T06:34:51.166953Z",
     "shell.execute_reply": "2025-05-27T06:34:51.165693Z",
     "shell.execute_reply.started": "2025-05-27T06:34:51.144505Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "rate_train = rate_train[['user_id', 'item_id', 'rating']].values\n",
    "rate_test = rate_test[['user_id', 'item_id', 'rating']].values\n",
    "\n",
    "# rate_train[:, :2] -= 1\n",
    "# rate_test[:, :2] -= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. User-User Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# class CF(object):\n",
    "#     def __init__(self, Y_data, k, dist_func=cosine_similarity, uuCF=1):\n",
    "#         self.uuCF = uuCF\n",
    "#         self.Y_data = Y_data if uuCF else Y_data[:, [1, 0, 2]]\n",
    "#         self.k = k\n",
    "#         self.dist_func = dist_func\n",
    "#         self.Ybar_data = None\n",
    "#         self.n_users = int(np.max(self.Y_data[:, 0])) + 1 \n",
    "#         self.n_items = int(np.max(self.Y_data[:, 1])) + 1\n",
    "    \n",
    "#     def add(self, new_data):\n",
    "#         self.Y_data = np.concatenate((self.Y_data, new_data), axis=0)\n",
    "\n",
    "#     def normalize_Y(self):\n",
    "#         users = self.Y_data[:, 0]\n",
    "#         self.Ybar_data = self.Y_data.copy()\n",
    "#         self.mu = np.zeros((self.n_users,))\n",
    "        \n",
    "#         for n in tqdm(range(self.n_users), desc=\"Normalizing users\"):\n",
    "#             ids = np.where(users == n)[0].astype(np.int32)\n",
    "#             ratings = self.Y_data[ids, 2]\n",
    "#             m = np.mean(ratings) \n",
    "#             if np.isnan(m):\n",
    "#                 m = 0\n",
    "#             self.mu[n] = m\n",
    "#             self.Ybar_data[ids, 2] = ratings - self.mu[n]\n",
    "\n",
    "#         self.Ybar = sparse.coo_matrix((self.Ybar_data[:, 2],\n",
    "#             (self.Ybar_data[:, 1], self.Ybar_data[:, 0])), (self.n_items, self.n_users))\n",
    "#         self.Ybar = self.Ybar.tocsr()\n",
    "\n",
    "#     def similarity(self):\n",
    "#         eps = 1e-6\n",
    "#         print(\"Computing similarity matrix...\")\n",
    "#         self.S = self.dist_func(self.Ybar.T, self.Ybar.T) + eps\n",
    "        \n",
    "#     def refresh(self):\n",
    "#         self.normalize_Y()\n",
    "#         self.similarity() \n",
    "        \n",
    "#     def fit(self):\n",
    "#         self.refresh()\n",
    "\n",
    "#     def __pred(self, u, i, normalized=1):\n",
    "#         ids = np.where(self.Y_data[:, 1] == i)[0].astype(np.int32)\n",
    "#         users_rated_i = (self.Y_data[ids, 0]).astype(np.int32)\n",
    "#         sim = self.S[u, users_rated_i]\n",
    "#         a = np.argsort(sim)[-self.k:] \n",
    "#         nearest_s = sim[a]\n",
    "#         r = self.Ybar[i, users_rated_i[a]]\n",
    "        \n",
    "#         if normalized:\n",
    "#             return (r*nearest_s)[0]/(np.abs(nearest_s).sum() + 1e-8)\n",
    "#         return (r*nearest_s)[0]/(np.abs(nearest_s).sum() + 1e-8) + self.mu[u]\n",
    "    \n",
    "#     def pred(self, u, i, normalized=1):\n",
    "#         if self.uuCF: \n",
    "#             return self.__pred(u, i, normalized)\n",
    "#         return self.__pred(i, u, normalized)\n",
    "    \n",
    "#     def recommend(self, u):\n",
    "#         ids = np.where(self.Y_data[:, 0] == u)[0]\n",
    "#         items_rated_by_u = self.Y_data[ids, 1].tolist()              \n",
    "#         recommended_items = []\n",
    "        \n",
    "#         for i in tqdm(range(self.n_items), desc=f\"Recommending for user {u}\", leave=False):\n",
    "#             if i not in items_rated_by_u:\n",
    "#                 rating = self.__pred(u, i)\n",
    "#                 if rating > 0: \n",
    "#                     recommended_items.append(i)\n",
    "        \n",
    "#         return recommended_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# rs = CF(rate_train, k = 30, uuCF = 1)\n",
    "# rs.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# train_rmse = calculate_rmse(rs, rate_train)\n",
    "# print(f'User-user CF, Train RMSE = {train_rmse:.4f}')\n",
    "\n",
    "# test_rmse = calculate_rmse(rs, rate_test)\n",
    "# print(f'User-user CF, Test RMSE = {test_rmse:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Item-Item Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# rs = CF(rate_train, k = 30, uuCF = 0)\n",
    "# rs.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# train_rmse = calculate_rmse(rs, rate_train)\n",
    "# print(f'User-user CF, Train RMSE = {train_rmse:.4f}')\n",
    "\n",
    "# test_rmse = calculate_rmse(rs, rate_test)\n",
    "# print(f'User-user CF, Test RMSE = {test_rmse:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Matrix Factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T06:34:51.169575Z",
     "iopub.status.busy": "2025-05-27T06:34:51.169125Z",
     "iopub.status.idle": "2025-05-27T06:34:51.189360Z",
     "shell.execute_reply": "2025-05-27T06:34:51.188265Z",
     "shell.execute_reply.started": "2025-05-27T06:34:51.169547Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MatrixFactorization(object):\n",
    "    def __init__(self, Y, K, lam=0.1, Xinit=None, Winit=None,\n",
    "                 learning_rate=0.5, max_iter=1000, print_every=100):\n",
    "        self.Y = Y # this is the utility matrix\n",
    "        self.K = K\n",
    "        self.lam = lam\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iter = max_iter\n",
    "        self.print_every = print_every\n",
    "        self.n_users = int(np.max(Y[:, 0])) + 1\n",
    "        self.n_items = int(np.max(Y[:, 1])) + 1\n",
    "        self.n_ratings = Y.shape[0]\n",
    "        self.X = np.random.randn(self.n_items, K) if Xinit is None else Xinit\n",
    "        self.W = np.random.randn(K, self.n_users) if Winit is None else Winit\n",
    "        self.b = np.random.randn(self.n_items) # item biases\n",
    "        self.d = np.random.randn(self.n_users)\n",
    "        \n",
    "    def loss(self):\n",
    "        L = 0\n",
    "        for i in range(self.n_ratings):\n",
    "            # user_id, item_id, rating\n",
    "            n, m, rating = int(self.Y[i, 0]), int(self.Y[i, 1]), self.Y[i, 2]\n",
    "            L += 0.5 * (self.X[m].dot(self.W[:, n]) + self.b[m] + self.d[n] - rating) ** 2  \n",
    "        L /= self.n_ratings # number of ratings\n",
    "        L_total = L + 0.5 * self.lam * (np.sum(self.X**2) + np.sum(self.W**2))\n",
    "        return L_total\n",
    "    \n",
    "    def updateXb(self):\n",
    "        for m in range(self.n_items):\n",
    "            # obtain all users who rated item m and get the corresponding ratings\n",
    "            ids = np.where(self.Y[:, 1] == m)[0] # row indices of items m\n",
    "            user_ids, ratings = self.Y[ids, 0].astype(np.int32), self.Y[ids, 2]\n",
    "            Wm, dm = self.W[:, user_ids], self.d[user_ids]\n",
    "            for i in range(30):\n",
    "                xm = self.X[m]\n",
    "                error = xm.dot(Wm) + self.b[m] + dm - ratings\n",
    "                grad_xm = error.dot(Wm.T) / self.n_ratings + self.lam * xm\n",
    "                grad_bm = np.sum(error) / self.n_ratings\n",
    "                self.X[m] -= self.learning_rate * grad_xm.reshape(-1)\n",
    "                self.b[m] -= self.learning_rate * grad_bm\n",
    "                \n",
    "    def updateWd(self):\n",
    "        for n in range(self.n_users):\n",
    "            # obtain all items rated by user n and get the corresponding ratings\n",
    "            ids = np.where(self.Y[:, 0] == n)[0]\n",
    "            item_ids, ratings = self.Y[ids, 1].astype(np.int32), self.Y[ids, 2]\n",
    "            Xn, bn = self.X[item_ids], self.b[item_ids]\n",
    "            for i in range(30):\n",
    "                wn = self.W[:, n]\n",
    "                error = Xn.dot(wn) + bn + self.d[n] - ratings\n",
    "                grad_wn = error.dot(Xn) / self.n_ratings + self.lam * wn\n",
    "                grad_dn = np.sum(error) / self.n_ratings     \n",
    "                self.W[:, n] -= self.learning_rate * grad_wn.reshape(-1)\n",
    "                self.d[n] -= self.learning_rate * grad_dn\n",
    "                \n",
    "    def fit(self):\n",
    "        for it in range(self.max_iter):\n",
    "            self.updateWd()\n",
    "            self.updateXb()\n",
    "            if (it + 1) % self.print_every == 0:\n",
    "                rmse_train = self.evaluate_RMSE(self.Y)\n",
    "                print('iter = %d, loss = %.4f, RMSE = %.4f' % (it + 1, self.loss(), rmse_train))\n",
    "                \n",
    "    def pred(self, user_id, item_id):\n",
    "        user_id, item_id = int(user_id), int(item_id)\n",
    "        pred = self.X[item_id, :].dot(self.W[:, user_id]) + self.b[item_id] + self.d[user_id]\n",
    "        return max(0, min(pred, 5))\n",
    "    \n",
    "    def evaluate_RMSE(self, rate_test):\n",
    "        n_tests = rate_test.shape[0]\n",
    "        SE = 0\n",
    "        for n in range(n_tests):\n",
    "            pred = self.pred(rate_test[n, 0], rate_test[n, 1])\n",
    "            SE += (pred - rate_test[n, 2]) ** 2     \n",
    "        RMSE = np.sqrt(SE / n_tests)\n",
    "        return RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-27T06:34:51.190898Z",
     "iopub.status.busy": "2025-05-27T06:34:51.190412Z",
     "iopub.status.idle": "2025-05-27T06:57:14.494758Z",
     "shell.execute_reply": "2025-05-27T06:57:14.493715Z",
     "shell.execute_reply.started": "2025-05-27T06:34:51.190833Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "rs = MatrixFactorization(rate_train, K = 30, lam = .01, print_every=1, learning_rate=50,\n",
    "max_iter = 10)\n",
    "rs.fit()\n",
    "\n",
    "# evaluate on test data\n",
    "RMSE = rs.evaluate_RMSE(rate_test)\n",
    "print(\"\\nMatrix Factorization CF, RMSE = %.4f\" %RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Metrics Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def calculate_metrics_sample_parallel(model, test_data, n_users=100, k_values=[10, 100], random_seed=42, n_processes=None):\n",
    "    if n_processes is None:\n",
    "        n_processes = min(mp.cpu_count(), 8)\n",
    "    \n",
    "    random.seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "    unique_users = np.unique(test_data[:, 0])\n",
    "    \n",
    "    if len(unique_users) <= n_users:\n",
    "        sample_users = unique_users\n",
    "    else:\n",
    "        sample_users = np.random.choice(unique_users, size=n_users, replace=False)\n",
    "    \n",
    "    args_list = [(user_id, model, test_data) for user_id in sample_users]\n",
    "    \n",
    "    with mp.Pool(processes=n_processes) as pool:\n",
    "        results = list(tqdm(\n",
    "            pool.imap(get_user_recommendations, args_list),\n",
    "            total=len(args_list),\n",
    "            desc=f\"Processing {n_users} random users\"\n",
    "        ))\n",
    "    \n",
    "    user_recommendations = {}\n",
    "    user_test_items = {}\n",
    "    \n",
    "    for user_id, recommended, relevant in results:\n",
    "        user_recommendations[user_id] = recommended[:max(k_values)]\n",
    "        user_test_items[user_id] = relevant\n",
    "    \n",
    "    return user_recommendations, user_test_items, sample_users\n",
    "\n",
    "\n",
    "def get_user_recommendations(args):\n",
    "    user_id, model, test_data = args\n",
    "    user_test_ratings = test_data[test_data[:, 0] == user_id]\n",
    "    relevant_items = user_test_ratings[user_test_ratings[:, 2] >= 4.0][:, 1].astype(int)\n",
    "    \n",
    "    try:\n",
    "        recommended_items = model.recommend(int(user_id))\n",
    "        return user_id, recommended_items, relevant_items\n",
    "    except:\n",
    "        return user_id, [], relevant_items\n",
    "\n",
    "def calculate_user_metrics(args):\n",
    "    user_id, recommended, relevant, k = args\n",
    "    \n",
    "    if len(relevant) == 0:\n",
    "        return None\n",
    "    \n",
    "    recommended_k = recommended[:k]\n",
    "    relevance_scores = [1 if item in relevant else 0 for item in recommended_k]\n",
    "    \n",
    "    ndcg_score = ndcg_at_k(relevance_scores, k)\n",
    "    hr_score = hit_rate_at_k(recommended, relevant, k)\n",
    "    precision_score = precision_at_k(recommended, relevant, k)\n",
    "    \n",
    "    return ndcg_score, hr_score, precision_score\n",
    "\n",
    "def dcg_at_k(r, k):\n",
    "    r = np.asfarray(r)[:k]\n",
    "    if r.size:\n",
    "        return np.sum(r / np.log2(np.arange(2, r.size + 2)))\n",
    "    return 0.\n",
    "\n",
    "def ndcg_at_k(r, k):\n",
    "    dcg_max = dcg_at_k(sorted(r, reverse=True), k)\n",
    "    if not dcg_max:\n",
    "        return 0.\n",
    "    return dcg_at_k(r, k) / dcg_max\n",
    "\n",
    "def hit_rate_at_k(recommended, relevant, k):\n",
    "    recommended_k = recommended[:k]\n",
    "    return len(set(recommended_k) & set(relevant)) > 0\n",
    "\n",
    "def precision_at_k(recommended, relevant, k):\n",
    "    recommended_k = recommended[:k]\n",
    "    hits = len(set(recommended_k) & set(relevant))\n",
    "    return hits / min(k, len(recommended_k)) if recommended_k else 0.0\n",
    "\n",
    "def personalization_at_k(all_recommendations, k):\n",
    "    recommendations_k = [rec[:k] for rec in all_recommendations]\n",
    "    total_pairs = 0\n",
    "    similar_pairs = 0\n",
    "    \n",
    "    for i in range(len(recommendations_k)):\n",
    "        for j in range(i + 1, len(recommendations_k)):\n",
    "            total_pairs += 1\n",
    "            overlap = len(set(recommendations_k[i]) & set(recommendations_k[j]))\n",
    "            if overlap > 0:\n",
    "                similar_pairs += 1\n",
    "    \n",
    "    return 1 - (similar_pairs / total_pairs) if total_pairs > 0 else 0.0\n",
    "\n",
    "def evaluate_recommendations_parallel(user_recommendations, user_test_items, k_values=[10, 100], n_processes=None):\n",
    "    if n_processes is None:\n",
    "        n_processes = min(mp.cpu_count(), 24)\n",
    "    \n",
    "    metrics = {}\n",
    "    all_recommendations = list(user_recommendations.values())\n",
    "    \n",
    "    for k in k_values:\n",
    "        args_list = [\n",
    "            (user_id, user_recommendations[user_id], user_test_items[user_id], k)\n",
    "            for user_id in user_recommendations.keys()\n",
    "        ]\n",
    "        \n",
    "        with mp.Pool(processes=n_processes) as pool:\n",
    "            results = list(tqdm(\n",
    "                pool.imap(calculate_user_metrics, args_list),\n",
    "                total=len(args_list),\n",
    "                desc=f\"Calculating metrics @{k}\"\n",
    "            ))\n",
    "        \n",
    "        valid_results = [r for r in results if r is not None]\n",
    "        \n",
    "        if valid_results:\n",
    "            ndcg_scores, hr_scores, precision_scores = zip(*valid_results)\n",
    "            metrics[f'NDCG@{k}'] = np.mean(ndcg_scores)\n",
    "            metrics[f'HR@{k}'] = np.mean(hr_scores)\n",
    "            metrics[f'Precision@{k}'] = np.mean(precision_scores)\n",
    "        else:\n",
    "            metrics[f'NDCG@{k}'] = 0.0\n",
    "            metrics[f'HR@{k}'] = 0.0\n",
    "            metrics[f'Precision@{k}'] = 0.0\n",
    "        \n",
    "        metrics[f'PSP@{k}'] = personalization_at_k(all_recommendations, k)\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def print_metrics_parallel(model, train_data, test_data, n_users=200):\n",
    "    user_recommendations, user_test_items, sample_users = calculate_metrics_sample_parallel(\n",
    "        model, test_data, n_users=n_users\n",
    "    )\n",
    "    \n",
    "    metrics = evaluate_recommendations_parallel(user_recommendations, user_test_items)\n",
    "    \n",
    "    print(f\"\\nMetrics computed on {n_users} random users (parallel):\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"NDCG@10: {metrics['NDCG@10']:.4f}\")\n",
    "    print(f\"HR@10: {metrics['HR@10']:.4f}\")\n",
    "    print(f\"Precision@10: {metrics['Precision@10']:.4f}\")\n",
    "    print(f\"PSP@10: {metrics['PSP@10']:.4f}\")\n",
    "    print(f\"NDCG@100: {metrics['NDCG@100']:.4f}\")\n",
    "    print(f\"HR@100: {metrics['HR@100']:.4f}\")\n",
    "    print(f\"Precision@100: {metrics['Precision@100']:.4f}\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    return metrics, sample_users\n",
    "\n",
    "print_metrics_parallel(rs, rate_train, rate_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "assert callable(calculate_metrics_sample_parallel), \"Parallel sample function not callable\"\n",
    "assert callable(evaluate_recommendations_parallel), \"Parallel evaluation function not callable\"\n",
    "# assert callable(calculate_rmse_parallel), \"Parallel RMSE function not callable\"\n",
    "assert callable(print_metrics_parallel), \"Parallel print function not callable\"\n",
    "assert mp.cpu_count() > 0, \"No CPU cores detected\"\n",
    "print(\"All parallel functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MovieLens1M\n",
    "## User-User Collaborative Filtering\n",
    "+ Train RMSE = 0.7215\n",
    "+ Test RMSE = 0.9234934723335452 (Noted)\n",
    "+ NDCG@10: 0.1533 (Noted)\n",
    "+ HR@10: 0.2800 (Noted)\n",
    "+ Precision@10: 0.0149 (Noted)\n",
    "+ Recall@10: 0.0213\n",
    "\n",
    "+ PSP@10: 0.0036 (Noted)\n",
    "+ NDCG@100: 0.1968 (Noted)\n",
    "+ HR@100: 0.6000 (Noted)\n",
    "+ Precision@100: 0.0340 (Exclude)\n",
    "\n",
    "## Item-Item Collaborative Filtering\n",
    "+ Train RMSE = 0.7097\n",
    "+ Test RMSE = 0.9010\n",
    "+ NDCG@10: 0.0661\n",
    "+ HR@10: 0.1200\n",
    "+ Precision@10: 0.0140\n",
    "+ PSP@10: 0.5455\n",
    "+ NDCG@100: 0.0919\n",
    "+ HR@100: 0.2600\n",
    "+ Precision@100: 0.0049\n",
    "\n",
    "## Matrix Factorization\n",
    "+ Train RMSE = 0.9069 (30 epochs)\n",
    "+ Test RMSE = 0.9200\n",
    "+ NDCG@10: 0.2108\n",
    "+ HR@10: 0.2901\n",
    "+ Precision@10: 0.0204\n",
    "+ Recall@10: 0.0231\n",
    "\n",
    "+ PSP@10: 0.3012\n",
    "+ NDCG@100: 0.2324\n",
    "+ HR@100: 0.4500\n",
    "+ Precision@100: 0.0405\n",
    "\n",
    "=============================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AmazonElectronics 500K\n",
    "\n",
    "## User-User Collaborative Filtering\n",
    "+ Train RMSE = 0.5052\n",
    "+ Test RMSE = 1.3143\n",
    "+ NDCG@10: 0.0008\n",
    "+ HR@10: 0.0014\n",
    "+ Precision@10: 0.0007\n",
    "+ PSP@10: 0.8934\n",
    "+ NDCG@100: 0.0009\n",
    "+ HR@100: 0.0074\n",
    "+ Precision@100: 0.0009\n",
    "\n",
    "## Item-Item Collaborative Filtering\n",
    "+ Train RMSE = 0.5379\n",
    "+ Test RMSE = 1.3366\n",
    "+ NDCG@10: 0.0008\n",
    "+ HR@10: 0.0010\n",
    "+ Precision@10: 0.0009\n",
    "+ PSP@10: 0.9873\n",
    "+ NDCG@100: 0.0013\n",
    "+ HR@100: 0.0085\n",
    "+ Precision@100: 0.0011\n",
    "\n",
    "\n",
    "## Matrix Factorization\n",
    "+ Train RMSE = 1.2748\n",
    "+ Test RMSE = 1.3987\n",
    "+ NDCG@10: 0.0010\n",
    "+ HR@10: 0.0019\n",
    "+ Precision@10: 0.0010\n",
    "+ PSP@10: 0.7494\n",
    "+ NDCG@100: 0.0018\n",
    "+ HR@100: 0.0090\n",
    "+ Precision@100: 0.0018"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7475228,
     "sourceId": 11963660,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7474913,
     "sourceId": 11965868,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
