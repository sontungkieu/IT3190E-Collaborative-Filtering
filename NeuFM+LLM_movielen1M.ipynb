{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11953048,"sourceType":"datasetVersion","datasetId":7514853}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q sentence-transformers wandb","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-27T16:11:19.178675Z","iopub.execute_input":"2025-05-27T16:11:19.179519Z","iopub.status.idle":"2025-05-27T16:12:32.330818Z","shell.execute_reply.started":"2025-05-27T16:11:19.179482Z","shell.execute_reply":"2025-05-27T16:12:32.330140Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m88.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Cell 1: Seed vÃ  imports\nimport os, random, math\nimport numpy as np\nimport pandas as pd\nfrom tqdm.auto import tqdm\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\n\nfrom sentence_transformers import SentenceTransformer\nimport wandb\n\n# --- Seed Ä‘á»ƒ tÃ¡i láº­p ---\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed_all(SEED)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark     = False\n\n# Device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device={device}, torch={torch.__version__}\")\n\n# W&B init\nName = \"NgocMinh\"\nModel_name = \"LightFM\"\nVersion = \"1.0.0\"\n\nwandb.login(key=\"62e3cf4c2815c959ed2609de1d55fa0504818c4a\")\n\n# 1.1. Khá»Ÿi táº¡o W&B run\nwandb.init(\n    project=\"hybrid-neumf-llm\",\n    name=\"HybridNeuMF_withLLM\",\n    config={\n        \"mf_dim\": 32,\n        \"mlp_layers\": [64, 32, 16, 8],\n        \"llm_model\": \"all-MiniLM-L6-v2\",  # ğŸŸ¢ CÃ“ dÃ²ng nÃ y\n        \"llm_dim\": 384,\n        \"batch_size\": 1024,\n        \"num_neg\": 4,\n        \"lr\": 1e-3,\n        \"weight_decay\": 1e-5,\n        \"epochs\": 10,\n        \"K\": 10\n    }\n)\ncfg = wandb.config\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T16:12:59.261302Z","iopub.execute_input":"2025-05-27T16:12:59.261610Z","iopub.status.idle":"2025-05-27T16:13:13.099119Z","shell.execute_reply.started":"2025-05-27T16:12:59.261583Z","shell.execute_reply":"2025-05-27T16:13:13.098597Z"}},"outputs":[{"name":"stdout","text":"Using device=cuda, torch=2.6.0+cu124\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkieusontung8\u001b[0m (\u001b[33mkieusontung8-hanoi-university-of-science-and-technology\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250527_161306-wh1wbpkh</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/kieusontung8-hanoi-university-of-science-and-technology/hybrid-neumf-llm/runs/wh1wbpkh' target=\"_blank\">HybridNeuMF_withLLM</a></strong> to <a href='https://wandb.ai/kieusontung8-hanoi-university-of-science-and-technology/hybrid-neumf-llm' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/kieusontung8-hanoi-university-of-science-and-technology/hybrid-neumf-llm' target=\"_blank\">https://wandb.ai/kieusontung8-hanoi-university-of-science-and-technology/hybrid-neumf-llm</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/kieusontung8-hanoi-university-of-science-and-technology/hybrid-neumf-llm/runs/wh1wbpkh' target=\"_blank\">https://wandb.ai/kieusontung8-hanoi-university-of-science-and-technology/hybrid-neumf-llm/runs/wh1wbpkh</a>"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"# Cell 2: Load ratings, movies, users\nratings = pd.read_csv(\"/kaggle/input/nicedataset/ratings.csv\")    # user_id,item_id,rating,timestamp\nmovies  = pd.read_csv(\"/kaggle/input/nicedataset/movies.csv\")     # item_id,title,genre,avg_rating,num_ratings,description\nusers   = pd.read_csv(\"/kaggle/input/nicedataset/users.csv\")      # user_id,gender,age,occupation,zip_code\n\n# 1) Label-encode user_id, item_id\nuser_enc = LabelEncoder(); ratings[\"uid\"] = user_enc.fit_transform(ratings.user_id)\nitem_enc = LabelEncoder(); ratings[\"iid\"] = item_enc.fit_transform(ratings.item_id)\nnum_users = ratings.uid.nunique()\nnum_items = ratings.iid.nunique()\n\n# 2) Encode user metadata:\n#   - gender: Mâ†’1, Fâ†’0\n#   - age: normalized [0,1]\n#   - occupation: one-hot\nusers[\"gender_bin\"] = (users.gender == \"M\").astype(int)\nmax_age = users.age.max()\nusers[\"age_norm\"] = users.age / max_age\nocc_ohe = pd.get_dummies(users.occupation, prefix=\"occ\")\nusers = pd.concat([users, occ_ohe], axis=1)\n# Build a matrix [num_users Ã— meta_dim]\nuser_meta = users.sort_values(\"user_id\").reset_index(drop=True)\nmeta_cols = [\"gender_bin\", \"age_norm\"] + list(occ_ohe.columns)\nuser_meta_matrix = torch.tensor(\n    user_meta[meta_cols].values.astype(np.float32),\n    device=device\n)  # shape: [num_users, meta_dim]\n\nprint(f\"Users={num_users}, Items={num_items}, meta_dim={user_meta_matrix.shape[1]}\")\n\n# 3) Train/test split theo timestamp (80/20)\nratings = ratings.sort_values(\"timestamp\")\ntrain_df, test_df = train_test_split(ratings, test_size=0.2, shuffle=False)\nprint(f\"Train interactions={len(train_df)}, Test interactions={len(test_df)}\")\n\n# 4) Chuáº©n bá»‹ text Ä‘á»ƒ encode LLM\nmovies[\"text_input\"] = (\n    movies.title.fillna(\"\") + \" \" +\n    movies.genre.fillna(\"\") + \" \" +\n    movies.description.fillna(\"\")\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T16:13:36.393072Z","iopub.execute_input":"2025-05-27T16:13:36.393432Z","iopub.status.idle":"2025-05-27T16:13:37.507668Z","shell.execute_reply.started":"2025-05-27T16:13:36.393406Z","shell.execute_reply":"2025-05-27T16:13:37.506851Z"}},"outputs":[{"name":"stdout","text":"Users=6040, Items=3706, meta_dim=23\nTrain interactions=800167, Test interactions=200042\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# === Cell Save 2b: LÆ°u táº¡m dá»¯ liá»‡u Ä‘Ã£ tiá»n xá»­ lÃ½ (sau Cell 2) ===\nimport pandas as pd\nimport torch\nimport gc\n\n# Giáº£ Ä‘á»‹nh cÃ¡c biáº¿n tá»« Cell 2: train_df, test_df, movies, users, user_meta_matrix\ntrain_df.to_parquet(\"train_df.parquet\")\ntest_df.to_parquet(\"test_df.parquet\")\nmovies.to_parquet(\"movies.parquet\")\nusers.to_parquet(\"users.parquet\")\ntorch.save(user_meta_matrix.cpu(), \"user_meta_matrix.pt\")\n\n# Giáº£i phÃ³ng bá»™ nhá»› nhá»¯ng biáº¿n lá»›n\ndel train_df, test_df, movies, users, user_meta_matrix\ngc.collect()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T16:13:41.710456Z","iopub.execute_input":"2025-05-27T16:13:41.710728Z","iopub.status.idle":"2025-05-27T16:13:42.548648Z","shell.execute_reply.started":"2025-05-27T16:13:41.710706Z","shell.execute_reply":"2025-05-27T16:13:42.547923Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"374"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"# === Cell Load 3a: Táº£i láº¡i dá»¯ liá»‡u trÆ°á»›c khi sinh LLM embedding (Cell 3) ===\nimport pandas as pd\nimport torch\n\n# Load láº¡i DataFrame vÃ  tensor metadata\ntrain_df = pd.read_parquet(\"train_df.parquet\")\ntest_df  = pd.read_parquet(\"test_df.parquet\")\nmovies   = pd.read_parquet(\"movies.parquet\")\nusers    = pd.read_parquet(\"users.parquet\")\n\n# Load láº¡i user metadata matrix\nuser_meta_matrix = torch.load(\"user_meta_matrix.pt\", map_location=device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T16:13:44.108560Z","iopub.execute_input":"2025-05-27T16:13:44.108822Z","iopub.status.idle":"2025-05-27T16:13:44.235666Z","shell.execute_reply.started":"2025-05-27T16:13:44.108800Z","shell.execute_reply":"2025-05-27T16:13:44.235151Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"\n\n# ğŸ›  Äá»“ng bá»™ thá»© tá»± item trong movies vá»›i item_enc.classes_\n# item_enc.classes_ chá»©a item_id (gá»‘c) Ä‘Ã£ Ä‘Æ°á»£c mÃ£ hÃ³a thÃ nh 0...num_items-1\nused_item_ids = item_enc.classes_\n\n# Sáº¯p xáº¿p láº¡i movies theo thá»© tá»± item_id Ä‘Ã£ mÃ£ hÃ³a\nmovies = movies.set_index(\"item_id\").loc[used_item_ids].reset_index()\nassert len(movies) == num_items  # Ä‘áº£m báº£o Ä‘Ãºng kÃ­ch thÆ°á»›c\n\n# Chuáº©n bá»‹ text input Ä‘á»ƒ encode\nitem_texts = (\n    movies[\"title\"].fillna(\"\") + \" \" +\n    movies[\"genre\"].fillna(\"\") + \" \" +\n    movies[\"description\"].fillna(\"\")\n).tolist()\n\n# Encode báº±ng LLM\nllm = SentenceTransformer(cfg.llm_model, device=device)\nitem_llm_emb = llm.encode(\n    item_texts,\n    batch_size=64,\n    show_progress_bar=True,\n    convert_to_numpy=True,\n    device=device\n)\n\n# Kiá»ƒm tra vÃ  convert sang tensor\nassert item_llm_emb.shape == (num_items, cfg.llm_dim), \"Sá»‘ lÆ°á»£ng hoáº·c chiá»u embedding khÃ´ng khá»›p\"\nitem_llm_emb = torch.tensor(item_llm_emb, dtype=torch.float32, device=device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T16:13:48.802276Z","iopub.execute_input":"2025-05-27T16:13:48.802975Z","iopub.status.idle":"2025-05-27T16:14:21.783458Z","shell.execute_reply.started":"2025-05-27T16:13:48.802952Z","shell.execute_reply":"2025-05-27T16:14:21.782918Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0608f2e338d448a7ba43e94b20fec2a5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"43af853e4b3e46a68934ccf9acd952e6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/10.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7be20bbd59684f4cb592289f3687d25a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3e00db9809b4a2fa4f9c12e62887278"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"51b470e053004101a287ee520b121b51"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a654da2703e42bb9beeb6f486fb6280"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8467a90e79c84306b6d566e98183251d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"59ab64d729484910b5ddfc4c1f67628c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d7516444490d4af9b338928ddecb3746"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"960e1fb5c053484b91db1f37dcb1cf01"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a6e8dc6e66494b519a16431aeb9b4c6e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/58 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e650022ae07d472fb7c7a2f47e8d3b2d"}},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"# === Cell Save 3b: LÆ°u táº¡m LLM embeddings (sau Cell 3) ===\nimport torch, gc\n\n# Giáº£ Ä‘á»‹nh biáº¿n item_llm_emb Ä‘Ã£ náº±m trÃªn GPU hoáº·c CPU\ntorch.save(item_llm_emb.cpu(), \"item_llm_emb.pt\")\n\n# Giáº£i phÃ³ng bá»™ nhá»›\ndel item_llm_emb\ngc.collect()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T16:14:27.459868Z","iopub.execute_input":"2025-05-27T16:14:27.460389Z","iopub.status.idle":"2025-05-27T16:14:27.904588Z","shell.execute_reply.started":"2025-05-27T16:14:27.460363Z","shell.execute_reply":"2025-05-27T16:14:27.904008Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"82"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"# === Cell Load 4a: Táº£i láº¡i LLM embedding trÆ°á»›c khi dÃ¹ng trong Dataset (Cell 4) ===\nimport torch\n\n# Load láº¡i embedding vÃ o Ä‘Ãºng device\nitem_llm_emb = torch.load(\"item_llm_emb.pt\", map_location=device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T16:14:30.109310Z","iopub.execute_input":"2025-05-27T16:14:30.109577Z","iopub.status.idle":"2025-05-27T16:14:30.118467Z","shell.execute_reply.started":"2025-05-27T16:14:30.109558Z","shell.execute_reply":"2025-05-27T16:14:30.117570Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Cell 4: Dataset with negative sampling + metadata + llm_emb\n# class HybridDataset(Dataset):\n#     def __init__(self, df, num_items, user_meta, llm_emb, num_neg=4):\n#         self.df        = df.reset_index(drop=True)\n#         self.num_items = num_items\n#         self.user_meta = user_meta\n#         self.llm_emb   = llm_emb\n#         self.num_neg   = num_neg\n#         self.pos_set   = set(zip(df.uid, df.iid))\n#         self.users, self.items, self.labels = self._prepare()\n\n#     def _prepare(self):\n#         U,I,L = [],[],[]\n#         for u,i in zip(self.df.uid, self.df.iid):\n#             U.append(u); I.append(i); L.append(1.0)\n#             for _ in range(self.num_neg):\n#                 neg = np.random.randint(self.num_items)\n#                 while (u, neg) in self.pos_set:\n#                     neg = np.random.randint(self.num_items)\n#                 U.append(u); I.append(neg); L.append(0.0)\n#         return U,I,L\n\n#     def __len__(self): return len(self.labels)\n\n#     def __getitem__(self, idx):\n#         u = torch.LongTensor([self.users[idx]])\n#         i = torch.LongTensor([self.items[idx]])\n#         l = torch.FloatTensor([self.labels[idx]])\n#         meta = self.user_meta[self.users[idx]].unsqueeze(0)  # [1,meta_dim]\n#         ll   = self.llm_emb[self.items[idx]].unsqueeze(0)   # [1,llm_dim]\n#         return u, i, meta, ll, l\n\n# train_ds = HybridDataset(train_df, num_items, user_meta_matrix, item_llm_emb, cfg.num_neg)\n# test_ds  = HybridDataset(test_df,  num_items, user_meta_matrix, item_llm_emb, 0)\n\n# train_loader = DataLoader(train_ds,\n#                           batch_size=cfg.batch_size,\n#                           shuffle=True,  num_workers=4)\n# test_loader  = DataLoader(test_ds,\n#                           batch_size=cfg.batch_size,\n#                           shuffle=False, num_workers=4)\n# === Cell 4: Dataset dÃ¹ng rating tháº­t (explicit feedback) ===\n# === Cell 4: Dataset dÃ¹ng rating tháº­t (explicit feedback) ===\nclass RatingDataset(Dataset):\n    def __init__(self, df, user_meta, item_llm_emb):\n        self.df = df.reset_index(drop=True)\n        self.users = df[\"uid\"].tolist()\n        self.items = df[\"iid\"].tolist()\n        self.ratings = df[\"rating\"].tolist()\n        self.user_meta = user_meta.cpu()       # Ä‘á»ƒ trÃ¡nh lá»—i multiprocess\n        self.item_llm = item_llm_emb.cpu()\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        u = torch.LongTensor([self.users[idx]])\n        i = torch.LongTensor([self.items[idx]])\n        r = torch.FloatTensor([self.ratings[idx]])   # dÃ¹ng rating tháº­t\n\n        meta = self.user_meta[self.users[idx]].unsqueeze(0)\n        ll   = self.item_llm[self.items[idx]].unsqueeze(0)\n        return u, i, meta, ll, r\n\n# Táº¡o Dataset + DataLoader khÃ´ng cáº§n negative sampling\ntrain_ds = RatingDataset(train_df, user_meta_matrix, item_llm_emb)\ntest_ds  = RatingDataset(test_df,  user_meta_matrix, item_llm_emb)\n\ntrain_loader = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True,  num_workers=0)\ntest_loader  = DataLoader(test_ds, batch_size=cfg.batch_size, shuffle=False, num_workers=0)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T16:14:32.558602Z","iopub.execute_input":"2025-05-27T16:14:32.559237Z","iopub.status.idle":"2025-05-27T16:14:32.702698Z","shell.execute_reply.started":"2025-05-27T16:14:32.559211Z","shell.execute_reply":"2025-05-27T16:14:32.701925Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# === Cell 5: MÃ´ hÃ¬nh NeuMF + Metadata + LLM cho Rating Prediction ===\nclass HybridNeuMF(nn.Module):\n    def __init__(self, n_users, n_items, mf_dim, mlp_layers, meta_dim, llm_dim):\n        super().__init__()\n        self.user_mf = nn.Embedding(n_users, mf_dim)\n        self.item_mf = nn.Embedding(n_items, mf_dim)\n\n        mlp_input = mlp_layers[0]\n        self.user_mlp = nn.Embedding(n_users, mlp_input // 2)\n        self.item_mlp = nn.Embedding(n_items, mlp_input // 2)\n\n        mlp_blocks = []\n        for in_d, out_d in zip(mlp_layers[:-1], mlp_layers[1:]):\n            mlp_blocks += [nn.Dropout(0.2), nn.Linear(in_d, out_d), nn.ReLU()]\n        self.mlp = nn.Sequential(*mlp_blocks)\n\n        fusion_dim = mf_dim + mlp_layers[-1] + meta_dim + llm_dim\n        self.predict = nn.Sequential(\n            nn.Linear(fusion_dim, 64),\n            nn.ReLU(),\n            nn.Linear(64, 1)      # âŒ khÃ´ng dÃ¹ng sigmoid\n        )\n\n    def forward(self, u, i, meta, ll):\n        mu = self.user_mf(u).squeeze(1)\n        mi = self.item_mf(i).squeeze(1)\n        mf_vec = mu * mi\n\n        xu = self.user_mlp(u).squeeze(1)\n        xi = self.item_mlp(i).squeeze(1)\n        mlp_vec = self.mlp(torch.cat([xu, xi], dim=1))\n\n        x = torch.cat([mf_vec, mlp_vec, meta.squeeze(1), ll.squeeze(1)], dim=1)\n        return self.predict(x)\n\n# Khá»Ÿi táº¡o model vÃ  loss\nmodel = HybridNeuMF(\n    num_users, num_items,\n    mf_dim=cfg.mf_dim,\n    mlp_layers=cfg.mlp_layers,\n    meta_dim=user_meta_matrix.shape[1],\n    llm_dim=cfg.llm_dim\n).to(device)\n\ncriterion = nn.MSELoss()   # âœ… DÃ¹ng MSELoss Ä‘á»ƒ dá»± Ä‘oÃ¡n rating\noptimizer = torch.optim.Adam(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n\nwandb.watch(model, log=\"all\", log_freq=100)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T16:14:35.440147Z","iopub.execute_input":"2025-05-27T16:14:35.440419Z","iopub.status.idle":"2025-05-27T16:14:35.461635Z","shell.execute_reply.started":"2025-05-27T16:14:35.440401Z","shell.execute_reply":"2025-05-27T16:14:35.461124Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"import math\nimport numpy as np\nimport torch\n\n# --- Top-K Ranking Metrics ---\ndef precision_at_k(ranked, truth, k):\n    return len(set(ranked[:k]) & set(truth)) / k\n\ndef recall_at_k(ranked, truth, k):\n    return len(set(ranked[:k]) & set(truth)) / len(truth) if len(truth) > 0 else 0.0\n\ndef ndcg_at_k(ranked, truth, k):\n    dcg = sum(1 / math.log2(idx + 2) for idx, item in enumerate(ranked[:k]) if item in truth)\n    idcg = sum(1 / math.log2(i + 2) for i in range(min(len(truth), k)))\n    return dcg / idcg if idcg > 0 else 0.0\n\ndef map_at_k(ranked, truth, k):\n    hits, s = 0, 0.0\n    for idx, item in enumerate(ranked[:k]):\n        if item in truth:\n            hits += 1\n            s += hits / (idx + 1)\n    return s / len(truth) if len(truth) > 0 else 0.0\n\ndef mrr_at_k(ranked, truth, k):\n    for idx, item in enumerate(ranked[:k]):\n        if item in truth:\n            return 1 / (idx + 1)\n    return 0.0\n\n# --- RMSE cho rating tháº­t (explicit feedback) ---\ndef rmse_real(model, loader):\n    model.eval()\n    se, n = 0.0, 0\n    with torch.no_grad():\n        for u, i, meta, ll, r in loader:\n            u, i = u.to(device), i.to(device)\n            meta, ll, r = meta.to(device), ll.to(device), r.to(device)\n            pred = model(u, i, meta, ll).view(-1)\n            r = r.view(-1)\n            se += ((pred - r) ** 2).sum().item()\n            n += r.size(0)\n    return math.sqrt(se / n)\n\n# --- HÃ m Ä‘Ã¡nh giÃ¡ toÃ n diá»‡n ---\n@torch.no_grad()\ndef evaluate_full(model, train_df, test_df, K, test_loader):\n    model.eval()\n\n    # Táº­p item Ä‘Ã£ tháº¥y (Ä‘á»ƒ mask)\n    train_map = train_df.groupby(\"uid\")[\"iid\"].apply(set).to_dict()\n    test_map  = test_df.groupby(\"uid\")[\"iid\"].apply(list).to_dict()\n\n    P_list, R_list, N_list, MAP_list, MRR_list = [], [], [], [], []\n\n    for u, truth in test_map.items():\n        if len(truth) == 0:\n            continue  # bá» qua user khÃ´ng cÃ³ ground truth\n\n        users = torch.LongTensor([u] * num_items).to(device)\n        items = torch.arange(num_items).to(device)\n        metas = user_meta_matrix[u].unsqueeze(0).repeat(num_items, 1).to(device)\n        lls   = item_llm_emb.to(device)\n\n        scores = model(users, items, metas.unsqueeze(1), lls.unsqueeze(1)).view(-1).cpu().numpy()\n\n        # Mask item Ä‘Ã£ tá»«ng tháº¥y trong train\n        for it in train_map.get(u, []):\n            scores[it] = -np.inf\n\n        ranked = np.argsort(-scores)\n\n        P_list.append(precision_at_k(ranked, truth, K))\n        R_list.append(recall_at_k(ranked, truth, K))\n        N_list.append(ndcg_at_k(ranked, truth, K))\n        MAP_list.append(map_at_k(ranked, truth, K))\n        MRR_list.append(mrr_at_k(ranked, truth, K))\n\n    return {\n        \"Precision@K\": np.mean(P_list),\n        \"Recall@K\":    np.mean(R_list),\n        \"NDCG@K\":      np.mean(N_list),\n        \"MAP@K\":       np.mean(MAP_list),\n        \"MRR@K\":       np.mean(MRR_list),\n        \"RMSE\":        rmse_real(model, test_loader)\n    }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T16:14:38.301175Z","iopub.execute_input":"2025-05-27T16:14:38.301794Z","iopub.status.idle":"2025-05-27T16:14:38.315409Z","shell.execute_reply.started":"2025-05-27T16:14:38.301771Z","shell.execute_reply":"2025-05-27T16:14:38.314671Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"results = []\nfor ep in range(1, cfg.epochs + 1):\n    model.train()\n    total_loss = 0.0\n\n    for u, i, meta, ll, r in tqdm(train_loader, desc=f\"Epoch {ep}\"):\n        u, i = u.to(device), i.to(device)\n        meta, ll, r = meta.to(device), ll.to(device), r.to(device)\n\n        optimizer.zero_grad()\n        pred = model(u, i, meta, ll).view(-1)\n        loss = criterion(pred, r.view(-1))\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item() * r.size(0)\n\n    train_loss = total_loss / len(train_ds)\n    metrics = evaluate_full(model, train_df, test_df, cfg.K, test_loader)\n    metrics[\"Train Loss\"] = train_loss\n    metrics[\"epoch\"] = ep\n\n    wandb.log(metrics)\n    results.append(metrics)\n\n    # ğŸ–¨ï¸ In toÃ n bá»™ cÃ¡c metric (trá»« epoch)\n    print(f\"Epoch {ep} â€” \" + \"  \".join(f\"{k}={v:.4f}\" for k, v in metrics.items() if k != \"epoch\"))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T16:14:41.409723Z","iopub.execute_input":"2025-05-27T16:14:41.410439Z","iopub.status.idle":"2025-05-27T16:20:36.044022Z","shell.execute_reply.started":"2025-05-27T16:14:41.410413Z","shell.execute_reply":"2025-05-27T16:20:36.043228Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Epoch 1:   0%|          | 0/782 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4f601702d854c0996867b3152fcb06f"}},"metadata":{}},{"name":"stdout","text":"Epoch 1 â€” Precision@K=0.0402  Recall@K=0.0045  NDCG@K=0.0379  MAP@K=0.0013  MRR@K=0.0858  RMSE=1.0303  Train Loss=1.6409\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 2:   0%|          | 0/782 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"82bb7820a4524143a80bc431afb962f2"}},"metadata":{}},{"name":"stdout","text":"Epoch 2 â€” Precision@K=0.0875  Recall@K=0.0104  NDCG@K=0.0826  MAP@K=0.0036  MRR@K=0.1464  RMSE=0.9961  Train Loss=1.0235\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 3:   0%|          | 0/782 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4223608d8388412ab5b016927efaf390"}},"metadata":{}},{"name":"stdout","text":"Epoch 3 â€” Precision@K=0.1168  Recall@K=0.0146  NDCG@K=0.1270  MAP@K=0.0067  MRR@K=0.2694  RMSE=0.9696  Train Loss=0.9290\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 4:   0%|          | 0/782 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c7f89036baa341fb9055ea9c3bd6044b"}},"metadata":{}},{"name":"stdout","text":"Epoch 4 â€” Precision@K=0.1350  Recall@K=0.0166  NDCG@K=0.1296  MAP@K=0.0067  MRR@K=0.2168  RMSE=0.9611  Train Loss=0.8889\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 5:   0%|          | 0/782 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"94ee04fbe45f41e9b634f20ce7633dec"}},"metadata":{}},{"name":"stdout","text":"Epoch 5 â€” Precision@K=0.0927  Recall@K=0.0110  NDCG@K=0.1018  MAP@K=0.0050  MRR@K=0.2313  RMSE=0.9604  Train Loss=0.8693\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 6:   0%|          | 0/782 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0372ba2ba15845d68e255ed058822058"}},"metadata":{}},{"name":"stdout","text":"Epoch 6 â€” Precision@K=0.1038  Recall@K=0.0142  NDCG@K=0.1079  MAP@K=0.0061  MRR@K=0.2164  RMSE=0.9590  Train Loss=0.8436\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 7:   0%|          | 0/782 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3aedd389a57749c7adb798c479a88049"}},"metadata":{}},{"name":"stdout","text":"Epoch 7 â€” Precision@K=0.1357  Recall@K=0.0195  NDCG@K=0.1511  MAP@K=0.0096  MRR@K=0.3249  RMSE=0.9617  Train Loss=0.7689\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 8:   0%|          | 0/782 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"98a1787aa37347d49c6d210c3a0526a4"}},"metadata":{}},{"name":"stdout","text":"Epoch 8 â€” Precision@K=0.1464  Recall@K=0.0205  NDCG@K=0.1519  MAP@K=0.0092  MRR@K=0.2837  RMSE=0.9797  Train Loss=0.6858\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 9:   0%|          | 0/782 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"487bc3c4d82f45cfac0b94b0878ac936"}},"metadata":{}},{"name":"stdout","text":"Epoch 9 â€” Precision@K=0.1384  Recall@K=0.0186  NDCG@K=0.1384  MAP@K=0.0078  MRR@K=0.2492  RMSE=0.9799  Train Loss=0.6262\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 10:   0%|          | 0/782 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc905bb095784fed885de42dbf3cad16"}},"metadata":{}},{"name":"stdout","text":"Epoch 10 â€” Precision@K=0.1514  Recall@K=0.0205  NDCG@K=0.1683  MAP@K=0.0095  MRR@K=0.3571  RMSE=0.9711  Train Loss=0.5863\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# ==== Cell: Inspect one batch ====\nbatch = next(iter(train_loader))\nprint(f\"Batch length: {len(batch)}\")\nfor i, t in enumerate(batch):\n    try:\n        print(f\"  idx {i}: shape={tuple(t.shape)}, dtype={t.dtype}\")\n    except AttributeError:          # cháº³ng háº¡n náº¿u pháº§n tá»­ lÃ  int/float\n        print(f\"  idx {i}: value={t}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T16:22:25.899382Z","iopub.execute_input":"2025-05-27T16:22:25.900111Z","iopub.status.idle":"2025-05-27T16:22:26.002621Z","shell.execute_reply.started":"2025-05-27T16:22:25.900068Z","shell.execute_reply":"2025-05-27T16:22:26.002003Z"}},"outputs":[{"name":"stdout","text":"Batch length: 5\n  idx 0: shape=(1024, 1), dtype=torch.int64\n  idx 1: shape=(1024, 1), dtype=torch.int64\n  idx 2: shape=(1024, 1, 23), dtype=torch.float32\n  idx 3: shape=(1024, 1, 384), dtype=torch.float32\n  idx 4: shape=(1024, 1), dtype=torch.float32\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# ==== Cell: Compute HR@K with metadata & LLM input ====\nK = 10\nn_neg = 100\nNUM_ITEMS = num_items  # tá»•ng sá»‘ item sau khi encode\n\ndef compute_hit_rate(model, data_loader, K=10, n_neg=100,\n                     device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n    \"\"\"\n    TÃ­nh HR@K cho model HybridNeuMF(user, item, meta, ll)\n    Dá»±a trÃªn batch cÃ³ 5 trÆ°á»ng:\n      [0] user_id (B,1), [1] pos_item (B,1), [2] user_meta (B,1,23), [3] item_llm (B,1,384), [4] rating\n    \"\"\"\n    model.eval()\n    hits, total = 0, 0\n\n    with torch.no_grad():\n        for batch in data_loader:\n            user      = batch[0].squeeze(1).to(device)     # (B,)\n            pos_item  = batch[1].squeeze(1).to(device)     # (B,)\n            user_meta = batch[2].squeeze(1).to(device)     # (B, 23)\n            item_llm  = batch[3].squeeze(1).to(device)     # (B, 384)\n\n            B = user.size(0)\n\n            # Sample negatives\n            neg_items = torch.randint(\n                0, NUM_ITEMS, size=(B, n_neg), device=device\n            )\n            dup_mask = (neg_items == pos_item.unsqueeze(1))\n            while dup_mask.any():\n                neg_items[dup_mask] = torch.randint(\n                    0, NUM_ITEMS, size=(dup_mask.sum(),), device=device\n                )\n                dup_mask = (neg_items == pos_item.unsqueeze(1))\n\n            # Candidates = [pos_item] + neg_items\n            candidates = torch.cat(\n                [pos_item.unsqueeze(1), neg_items], dim=1\n            )  # (B, n_neg + 1)\n\n            users_flat = user.unsqueeze(1).expand_as(candidates).reshape(-1)\n            items_flat = candidates.reshape(-1)\n\n            # Má»Ÿ rá»™ng user_meta & item_llm tÆ°Æ¡ng á»©ng (broadcast Ä‘Ãºng chiá»u)\n            meta_expanded = user_meta.unsqueeze(1).expand(B, n_neg + 1, -1).reshape(-1, user_meta.shape[-1])\n            llm_expanded  = item_llm.unsqueeze(1).expand(B, n_neg + 1, -1).reshape(-1, item_llm.shape[-1])\n\n            # Gá»i forward\n            scores = model(users_flat, items_flat, meta_expanded, llm_expanded)\n            scores = scores.reshape(candidates.shape)\n\n            _, topk_idx = scores.topk(K, dim=1)\n            hits += (topk_idx == 0).any(dim=1).sum().item()\n            total += B\n\n    return hits / total if total else 0.0\n\n# ğŸ” ÄÃ¡nh giÃ¡ HR@10\nhrK_train = compute_hit_rate(model, train_loader, K=K, n_neg=n_neg)\nhrK_test  = compute_hit_rate(model, test_loader,  K=K, n_neg=n_neg)\n\nprint(f\"âœ… HR@{K} on train set: {hrK_train:.4f}\")\nprint(f\"âœ… HR@{K} on test  set: {hrK_test :.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T16:24:53.230445Z","iopub.execute_input":"2025-05-27T16:24:53.231184Z","iopub.status.idle":"2025-05-27T16:25:33.097387Z","shell.execute_reply.started":"2025-05-27T16:24:53.231157Z","shell.execute_reply":"2025-05-27T16:25:33.096665Z"}},"outputs":[{"name":"stdout","text":"âœ… HR@10 on train set: 0.3159\nâœ… HR@10 on test  set: 0.2743\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# Cell 9: Show & save\ndf_res = pd.DataFrame(results).set_index(\"epoch\")\ndisplay(df_res)\ntorch.save(model.state_dict(), \"/kaggle/working/hybrid_neumf_llm.pth\")\ndf_res.to_csv(\"/kaggle/working/hybrid_neumf_llm_metrics.csv\")\nprint(\"âœ… Saved model & metrics.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-26T14:07:53.220914Z","iopub.execute_input":"2025-05-26T14:07:53.221437Z","iopub.status.idle":"2025-05-26T14:07:53.256790Z","shell.execute_reply.started":"2025-05-26T14:07:53.221414Z","shell.execute_reply":"2025-05-26T14:07:53.256247Z"}},"outputs":[],"execution_count":null}]}