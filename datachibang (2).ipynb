{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11964209,"sourceType":"datasetVersion","datasetId":7523175},{"sourceId":11987456,"sourceType":"datasetVersion","datasetId":7516351},{"sourceId":12046086,"sourceType":"datasetVersion","datasetId":7548335}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Cell 1: Cài đặt (chạy 1 lần)\n!pip install -q sentence-transformers wandb","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-05T03:54:19.108714Z","iopub.execute_input":"2025-06-05T03:54:19.109263Z","iopub.status.idle":"2025-06-05T03:55:34.020488Z","shell.execute_reply.started":"2025-06-05T03:54:19.109240Z","shell.execute_reply":"2025-06-05T03:55:34.019624Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m72.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Cell 2: Imports, seed, device & W&B init\nimport os, random, math\nimport numpy as np\nimport pandas as pd\nfrom tqdm.auto import tqdm\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sentence_transformers import SentenceTransformer\nimport wandb\n\n# --- Seed để tái lập ---\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed_all(SEED)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark     = False\n\n# Thiết lập tokenizer parallelism để tránh warning\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\n# Device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device={device}\")\n\nwandb.finish()  \n# W&B init\nName = \"NgocMinh\"\nModel_name = \"NeuFM + LLM\"\nVersion = \"1.0.0\"\n\nwandb.login(key=\"62e3cf4c2815c959ed2609de1d55fa0504818c4a\")\n\n# 1.1. Khởi tạo W&B run\nwandb.init(\n    entity=\"IT3190E-20242-MachinLearning\",\n    project=\"cf-electronics\",\n    name=\"HybridNeuMF_NewDS\",\n    config={\n        \"mf_dim\": 32,\n        \"mlp_layers\": [64, 32, 16, 8],\n        \"llm_model\": \"all-MiniLM-L6-v2\",\n        \"llm_dim\": 384,\n        \"batch_size\": 1024,\n        \"lr\": 1e-3,\n        \"weight_decay\": 1e-4,\n        \"epochs\": 15,\n        \"K\": 20\n    }\n)\ncfg = wandb.config\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T03:55:34.022132Z","iopub.execute_input":"2025-06-05T03:55:34.022354Z","iopub.status.idle":"2025-06-05T03:55:46.582908Z","shell.execute_reply.started":"2025-06-05T03:55:34.022334Z","shell.execute_reply":"2025-06-05T03:55:46.582352Z"}},"outputs":[{"name":"stdout","text":"Using device=cuda\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkieusontung8\u001b[0m (\u001b[33mkieusontung8-hanoi-university-of-science-and-technology\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250605_035540-7n5wd4jn</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/IT3190E-20242-MachinLearning/cf-electronics/runs/7n5wd4jn' target=\"_blank\">HybridNeuMF_NewDS</a></strong> to <a href='https://wandb.ai/IT3190E-20242-MachinLearning/cf-electronics' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/IT3190E-20242-MachinLearning/cf-electronics' target=\"_blank\">https://wandb.ai/IT3190E-20242-MachinLearning/cf-electronics</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/IT3190E-20242-MachinLearning/cf-electronics/runs/7n5wd4jn' target=\"_blank\">https://wandb.ai/IT3190E-20242-MachinLearning/cf-electronics/runs/7n5wd4jn</a>"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/ulti-chiu-cui-ri/train_ratings (5).csv\")   # user_id,item_id,rating,...\ntest_df  = pd.read_csv(\"/kaggle/input/ulti-chiu-cui-ri/test_ratings (4).csv\")\nmeta_df  = pd.read_csv(\"/kaggle/input/ulti-chiu-cui-ri/filtered_metadata (2).csv\")\nprint(len(train_df[\"item_id\"].unique()), len(test_df[\"item_id\"].unique()), len(meta_df[\"item_id\"].unique()))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T03:55:46.583601Z","iopub.execute_input":"2025-06-05T03:55:46.583841Z","iopub.status.idle":"2025-06-05T03:55:47.264048Z","shell.execute_reply.started":"2025-06-05T03:55:46.583812Z","shell.execute_reply":"2025-06-05T03:55:47.263469Z"}},"outputs":[{"name":"stdout","text":"2357 1126 2401\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Cell 3: Load & encode IDs\ntrain_df = pd.read_csv(\"/kaggle/input/ulti-chiu-cui-ri/train_ratings (5).csv\")   # user_id,item_id,rating,...\ntest_df  = pd.read_csv(\"/kaggle/input/ulti-chiu-cui-ri/test_ratings (4).csv\")\nmeta_df  = pd.read_csv(\"/kaggle/input/ulti-chiu-cui-ri/filtered_metadata (2).csv\")\n\n# Fit trên toàn bộ user_id từ train + test\nuser_enc = LabelEncoder()\nuser_enc.fit(pd.concat([train_df[\"user_id\"], test_df[\"user_id\"]], ignore_index=True))\ntrain_df[\"uid\"] =  user_enc.transform(train_df[\"user_id\"])\ntest_df[\"uid\"]  = user_enc.transform(test_df[\"user_id\"])\n\n# Fit item_id từ train + test + metadata\nitem_enc = LabelEncoder()\nitem_enc.fit(pd.concat([train_df[\"item_id\"], test_df[\"item_id\"], meta_df[\"item_id\"]], ignore_index=True))\ntrain_df[\"iid\"] = item_enc.transform(train_df[\"item_id\"])\ntest_df[\"iid\"]  = item_enc.transform(test_df[\"item_id\"])\nmeta_df[\"iid\"]  = item_enc.transform(meta_df[\"item_id\"])\n\n# Lấy đúng số lượng class (không dùng .nunique())\nnum_users = len(user_enc.classes_)\nnum_items = len(item_enc.classes_)\n\nprint(f\"✅ Encoded: {num_users} users, {num_items} items\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T03:55:47.265450Z","iopub.execute_input":"2025-06-05T03:55:47.265659Z","iopub.status.idle":"2025-06-05T03:55:47.672437Z","shell.execute_reply.started":"2025-06-05T03:55:47.265643Z","shell.execute_reply":"2025-06-05T03:55:47.671887Z"}},"outputs":[{"name":"stdout","text":"✅ Encoded: 1289 users, 2401 items\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Cell 4: Prepare item text for LLM (concat description, features, cat egories)\nmeta_df[\"text_input\"] = (\n    meta_df[\"description\"].fillna(\"\") + \" \" +\n    meta_df[\"features\"].fillna(\"\")    + \" \" +\n    meta_df[\"categories\"].fillna(\"\")\n)\n\n# Reindex metadata so that row i corresponds to iid = i\nmeta_df = meta_df.set_index(\"iid\").reindex(range(num_items)).fillna(\"\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T03:55:47.673180Z","iopub.execute_input":"2025-06-05T03:55:47.673429Z","iopub.status.idle":"2025-06-05T03:55:47.698697Z","shell.execute_reply.started":"2025-06-05T03:55:47.673405Z","shell.execute_reply":"2025-06-05T03:55:47.697996Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"\n\n# Khởi tạo mô hình LLM (không truyền device để tránh lỗi CUDA)\nllm = SentenceTransformer(cfg.llm_model)\n\n# Chuẩn bị input văn bản cho từng item\nitem_texts = meta_df[\"text_input\"].tolist()  # length == num_items\n\n# Encode bằng LLM (trả về numpy), dùng GPU mặc định nếu có\nitem_emb_np = llm.encode(\n    item_texts,\n    batch_size=64,\n    show_progress_bar=True,\n    convert_to_numpy=True  # trả kết quả là numpy array\n)\n\n# Đảm bảo đúng shape đầu ra\nassert item_emb_np.shape == (num_items, cfg.llm_dim)\n\n# Chuyển sang torch tensor & lên GPU (nếu dùng)\nitem_llm_emb = torch.tensor(item_emb_np, dtype=torch.float32).to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T03:55:47.699479Z","iopub.execute_input":"2025-06-05T03:55:47.699709Z","iopub.status.idle":"2025-06-05T03:55:59.200873Z","shell.execute_reply.started":"2025-06-05T03:55:47.699689Z","shell.execute_reply":"2025-06-05T03:55:59.200306Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90fdc842b7484bd1beed763997e5abb2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d92bf72406a4db1834fec877324508b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/10.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e8b05ffd4ad44480a452cb2865c0a2da"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19450b08c7fd451695a6ae47c83b8904"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc26dd67b8b84c9eb12cd11e08d52c34"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14489f428abe48fc8cafc83b02678da9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f0e7a5a3b434945b6fb9601b2493560"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b8b555cbbf84e5a95374afa8857fbc2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e55c59a5dc64134899c33c50b407989"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ffd25bddf4f482f96fb0cf5826b80a5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1628c482591542978876525c6ad18b90"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/38 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"402202abe426442ab8cced2aed3e517f"}},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"print(item_llm_emb)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T03:55:59.201626Z","iopub.execute_input":"2025-06-05T03:55:59.201982Z","iopub.status.idle":"2025-06-05T03:55:59.415268Z","shell.execute_reply.started":"2025-06-05T03:55:59.201955Z","shell.execute_reply":"2025-06-05T03:55:59.414633Z"}},"outputs":[{"name":"stdout","text":"tensor([[-0.0760,  0.0591, -0.0340,  ...,  0.0379,  0.0162,  0.0475],\n        [-0.1133,  0.0201,  0.0159,  ..., -0.0125,  0.0535,  0.0797],\n        [-0.1383,  0.0015, -0.1034,  ..., -0.0211,  0.0814,  0.0266],\n        ...,\n        [-0.0370,  0.0337, -0.0654,  ...,  0.0287, -0.0300,  0.0163],\n        [ 0.0277, -0.0755,  0.0045,  ...,  0.0337, -0.0867,  0.0918],\n        [-0.0824,  0.0717, -0.0199,  ..., -0.0560, -0.0052,  0.0227]],\n       device='cuda:0')\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# === Cell 4.5: Tạo embedding cho toàn bộ user từ review_text ===\n\n# B1: Gộp review text theo user_id (chỉ user có review)\nuser_texts_partial = (\n    train_df.groupby(\"user_id\")[\"text\"]\n    .apply(lambda x: \" \".join(x.dropna().astype(str)))\n    .reset_index()\n)\n\n# B2: Gắn uid theo LabelEncoder\nuser_texts_partial[\"uid\"] = user_enc.transform(user_texts_partial[\"user_id\"])\n\n# B3: Tạo mảng văn bản với số lượng = num_users, default = \"\"\nuser_text_array = [\"\"] * num_users\nuser_text_dict = dict(zip(user_texts_partial[\"uid\"], user_texts_partial[\"text\"]))\n\nfor uid in range(num_users):\n    user_text_array[uid] = user_text_dict.get(uid, \"\")  # fallback nếu không có review\n\n# B4: Encode bằng SentenceTransformer\nuser_llm_np = llm.encode(user_text_array, batch_size=64, show_progress_bar=True)\nuser_llm_emb = torch.tensor(user_llm_np, dtype=torch.float32).to(device) # user_id's text được encode và chuyển sang tensor\n\n# B5: Kiểm tra shape\nassert user_llm_emb.shape == (num_users, cfg.llm_dim)\nprint(\"✅ user_llm_emb shape:\", user_llm_emb.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T03:55:59.416107Z","iopub.execute_input":"2025-06-05T03:55:59.416513Z","iopub.status.idle":"2025-06-05T03:56:09.012406Z","shell.execute_reply.started":"2025-06-05T03:55:59.416489Z","shell.execute_reply":"2025-06-05T03:56:09.011624Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/21 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9df9a7c1ad944229bd8a830b11151d3"}},"metadata":{}},{"name":"stdout","text":"✅ user_llm_emb shape: torch.Size([1289, 384])\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# === Cell 4.6: Lưu lại các embedding đã tính sau Cell 4.5 ===\n\nsave_dir = \"/kaggle/working\"  # Hoặc \"./\" nếu chạy local notebook\n\ntorch.save(item_llm_emb.cpu(), f\"{save_dir}/item_llm_emb.pt\")\ntorch.save(user_llm_emb.cpu(), f\"{save_dir}/user_llm_emb.pt\")\n\nprint(\"✅ Đã lưu: item_llm_emb.pt và user_llm_emb.pt\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T03:56:09.013360Z","iopub.execute_input":"2025-06-05T03:56:09.013629Z","iopub.status.idle":"2025-06-05T03:56:09.033088Z","shell.execute_reply.started":"2025-06-05T03:56:09.013600Z","shell.execute_reply":"2025-06-05T03:56:09.032336Z"}},"outputs":[{"name":"stdout","text":"✅ Đã lưu: item_llm_emb.pt và user_llm_emb.pt\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# === Load lại các embedding đã lưu (để tránh tính lại mỗi lần) ===\n\nitem_llm_emb = torch.load(f\"{save_dir}/item_llm_emb.pt\").to(device)\nuser_llm_emb = torch.load(f\"{save_dir}/user_llm_emb.pt\").to(device)\n\nprint(\"✅ Đã load embedding vào RAM & đưa lên GPU\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T03:56:09.035968Z","iopub.execute_input":"2025-06-05T03:56:09.036186Z","iopub.status.idle":"2025-06-05T03:56:09.288761Z","shell.execute_reply.started":"2025-06-05T03:56:09.036163Z","shell.execute_reply":"2025-06-05T03:56:09.287916Z"}},"outputs":[{"name":"stdout","text":"✅ Đã load embedding vào RAM & đưa lên GPU\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# === Cell 6: Dataset explicit rating (thêm user embedding) ===\nclass RatingDataset(Dataset):\n    def __init__(self, df, item_emb, user_emb):\n        self.uids = df[\"uid\"].values\n        self.iids = df[\"iid\"].values\n        self.ratings = df[\"rating\"].values.astype(np.float32)\n        self.item_emb = item_emb.cpu()\n        self.user_emb = user_emb.cpu()\n\n    def __len__(self): return len(self.uids)\n\n    def __getitem__(self, idx):\n        u = torch.LongTensor([self.uids[idx]])\n        i = torch.LongTensor([self.iids[idx]])\n        r = torch.FloatTensor([self.ratings[idx]])\n        ll_item = self.item_emb[self.iids[idx]].unsqueeze(0)\n        ll_user = self.user_emb[self.uids[idx]].unsqueeze(0)\n        return u, i, ll_user, ll_item, r\n\n# Khởi tạo loader như thường lệ\ntrain_ds = RatingDataset(train_df, item_llm_emb, user_llm_emb)\ntest_ds  = RatingDataset(test_df,  item_llm_emb, user_llm_emb)\n\ntrain_loader = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True, num_workers=0)\ntest_loader  = DataLoader(test_ds,  batch_size=cfg.batch_size, shuffle=False, num_workers=0)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T03:56:09.289426Z","iopub.execute_input":"2025-06-05T03:56:09.289648Z","iopub.status.idle":"2025-06-05T03:56:09.300268Z","shell.execute_reply.started":"2025-06-05T03:56:09.289631Z","shell.execute_reply":"2025-06-05T03:56:09.299512Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# === Tính sampling_probs theo popularity-biased negative sampling ===\nitem_counts = train_df['iid'].value_counts().sort_index()\n\ncounts_tensor = torch.zeros(num_items)\ncounts_tensor[item_counts.index] = torch.tensor(item_counts.values, dtype=torch.float32)\n\nsampling_probs = counts_tensor.pow(0.75)\nsampling_probs /= sampling_probs.sum()  # chuẩn hóa về tổng 1\nsampling_probs = sampling_probs.to(device)\n\n# === Dataset Class ===\nclass ImplicitDataset(Dataset):\n    def __init__(self, df_pos, num_items, item_emb, user_emb, sampling_probs, num_neg=4):\n        self.pos = df_pos[[\"uid\", \"iid\"]].values\n        self.user_pos_dict = df_pos.groupby(\"uid\")[\"iid\"].apply(set).to_dict()\n        self.num_items = num_items\n        self.num_neg = num_neg\n        self.item_emb = item_emb.to(device)\n        self.user_emb = user_emb.to(device)\n        self.sampling_probs = sampling_probs\n\n    def __len__(self):\n        return len(self.pos)\n\n    def __getitem__(self, idx):\n        u, i_pos = self.pos[idx]\n        triplets = [(u, i_pos, 1.0)]\n        neg = 0\n        while neg < self.num_neg:\n            i_neg = torch.multinomial(self.sampling_probs, 1).item()\n            if i_neg not in self.user_pos_dict.get(u, set()):\n                triplets.append((u, i_neg, 0.0))\n                neg += 1\n\n        out = []\n        for uu, ii, label in triplets:\n            out.append((\n                torch.tensor(uu, dtype=torch.long),\n                torch.tensor(ii, dtype=torch.long),\n                self.user_emb[uu],\n                self.item_emb[ii],\n                torch.tensor(label, dtype=torch.float32)\n            ))\n        return out\n\n# === Collate function ===\ndef collate_triplets(batch):\n    u, i, lu, li, l = zip(*[t for sub in batch for t in sub])\n    return (\n        torch.stack(u),     # [batch_size]\n        torch.stack(i),     # [batch_size]\n        torch.stack(lu),    # [batch_size, emb_dim]\n        torch.stack(li),    # [batch_size, emb_dim]\n        torch.stack(l)      # [batch_size]\n    )\n\n# === Tạo DataLoader ===\ntrain_ds = ImplicitDataset(\n    train_df, num_items,\n    item_llm_emb, user_llm_emb,\n    sampling_probs=sampling_probs,\n    num_neg=4\n)\n\ntrain_loader = DataLoader(\n    train_ds,\n    batch_size=cfg.batch_size,\n    shuffle=True,\n    collate_fn=collate_triplets,\n    num_workers=0\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T03:56:09.301205Z","iopub.execute_input":"2025-06-05T03:56:09.301454Z","iopub.status.idle":"2025-06-05T03:56:09.349091Z","shell.execute_reply.started":"2025-06-05T03:56:09.301429Z","shell.execute_reply":"2025-06-05T03:56:09.348610Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# # === Cell 7: HybridNeuMF mở rộng với ll_user + ll_item ===\n# class HybridNeuMF(nn.Module):\n#     def __init__(self, n_users, n_items, mf_dim, mlp_layers, llm_dim):\n#         super().__init__()\n#         self.user_mf = nn.Embedding(n_users, mf_dim)\n#         self.item_mf = nn.Embedding(n_items, mf_dim)\n#         self.user_mlp = nn.Embedding(n_users, mlp_layers[0] // 2)\n#         self.item_mlp = nn.Embedding(n_items, mlp_layers[0] // 2)\n\n#         blocks = []\n#         for d_in, d_out in zip(mlp_layers[:-1], mlp_layers[1:]):\n#             blocks += [nn.Dropout(0.2), nn.Linear(d_in, d_out), nn.ReLU()]\n#         self.mlp = nn.Sequential(*blocks)\n\n#         fusion_dim = mf_dim + mlp_layers[-1] + 2 * llm_dim\n#         self.predict = nn.Sequential(\n#             nn.Linear(fusion_dim, 64),\n#             nn.ReLU(),\n#             nn.Linear(64, 1)\n#         )\n\n#     def forward(self, u, i, ll_user, ll_item):\n#         mu = self.user_mf(u).squeeze(1)\n#         mi = self.item_mf(i).squeeze(1)\n#         cf_vec = mu * mi\n\n#         xu = self.user_mlp(u).squeeze(1)\n#         xi = self.item_mlp(i).squeeze(1)\n#         mlp_vec = self.mlp(torch.cat([xu, xi], dim=1))\n\n#         x = torch.cat([cf_vec, mlp_vec, ll_user.squeeze(1), ll_item.squeeze(1)], dim=1)\n#         return self.predict(x)\nimport torch\nimport torch.nn as nn\n\nclass HybridNeuMF(nn.Module):\n    def __init__(self, n_users, n_items, mf_dim, mlp_layers, llm_dim):\n        super().__init__()\n\n        # ID embeddings\n        self.user_mf = nn.Embedding(n_users, mf_dim)\n        self.item_mf = nn.Embedding(n_items, mf_dim)\n        self.user_mlp = nn.Embedding(n_users, mlp_layers[0] // 2)\n        self.item_mlp = nn.Embedding(n_items, mlp_layers[0] // 2)\n\n        # MLP pathway\n        blocks = []\n        for d_in, d_out in zip(mlp_layers[:-1], mlp_layers[1:]):\n            blocks += [nn.Dropout(0.2), nn.Linear(d_in, d_out), nn.ReLU()]\n        self.mlp = nn.Sequential(*blocks)\n\n        # Fusion dimensions\n        self.zid_dim = mf_dim + mlp_layers[-1]\n        self.zsem_dim = 2 * llm_dim\n        self.fusion_input_dim = self.zid_dim + self.zsem_dim\n\n        # Projection & gating\n        self.zsem_project = nn.Linear(self.zsem_dim, self.zid_dim)\n        self.gate_layer = nn.Linear(self.fusion_input_dim, 1)\n\n        # Final prediction\n        self.final_layer = nn.Sequential(\n            nn.Linear(self.zid_dim, 64),\n            nn.ReLU(),\n            nn.Linear(64, 1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, u, i, ll_user, ll_item):\n        # MF vector\n        mu = self.user_mf(u).squeeze(1)\n        mi = self.item_mf(i).squeeze(1)\n        cf_vec = mu * mi\n\n        # MLP vector\n        xu = self.user_mlp(u).squeeze(1)\n        xi = self.item_mlp(i).squeeze(1)\n        mlp_input = torch.cat([xu, xi], dim=1)\n        mlp_vec = self.mlp(mlp_input)\n\n        # ID-based vector\n        z_id = torch.cat([cf_vec, mlp_vec], dim=1)  # [B, zid_dim]\n\n        # LLM-based semantic vector\n        z_sem = torch.cat([ll_user, ll_item], dim=1)  # [B, zsem_dim]\n        z_sem_proj = self.zsem_project(z_sem)         # [B, zid_dim]\n\n        # Gated fusion\n        gate_input = torch.cat([z_id, z_sem], dim=1)  # [B, fusion_input_dim]\n        alpha = torch.sigmoid(self.gate_layer(gate_input))  # [B, 1]\n        z_amz = alpha * z_id + (1 - alpha) * z_sem_proj      # ✅ DÙNG `z_sem_proj`\n\n        # Final prediction\n        out = self.final_layer(z_amz).squeeze(1)\n        return out\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T03:56:09.349850Z","iopub.execute_input":"2025-06-05T03:56:09.350073Z","iopub.status.idle":"2025-06-05T03:56:09.360283Z","shell.execute_reply.started":"2025-06-05T03:56:09.350057Z","shell.execute_reply":"2025-06-05T03:56:09.359586Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"print(cfg.llm_dim)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T03:56:09.361086Z","iopub.execute_input":"2025-06-05T03:56:09.361329Z","iopub.status.idle":"2025-06-05T03:56:09.375791Z","shell.execute_reply.started":"2025-06-05T03:56:09.361304Z","shell.execute_reply":"2025-06-05T03:56:09.375196Z"}},"outputs":[{"name":"stdout","text":"384\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# === Cell 7.5: Khởi tạo model, optimizer (có regularization), loss ===\n\nmodel = HybridNeuMF(\n    n_users=num_users,\n    n_items=num_items,\n    mf_dim=cfg.mf_dim,\n    mlp_layers=cfg.mlp_layers,\n    llm_dim=cfg.llm_dim\n).to(device)\n\n# ✅ Thêm regularization: weight_decay > 0\noptimizer = torch.optim.Adam(\n    model.parameters(),\n    lr=cfg.lr,\n    weight_decay=cfg.weight_decay  # ví dụ: 1e-4 để L2 regularization\n)\n\ncriterion = criterion = nn.BCEWithLogitsLoss()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T03:56:09.376452Z","iopub.execute_input":"2025-06-05T03:56:09.376648Z","iopub.status.idle":"2025-06-05T03:56:09.396136Z","shell.execute_reply.started":"2025-06-05T03:56:09.376634Z","shell.execute_reply":"2025-06-05T03:56:09.395610Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# # === Cell 8: Metric (precision@K, recall@K, ndcg@K...) ===\n# def precision_at_k(ranked, truth, k): return len(set(ranked[:k]) & set(truth)) / k\n\n# def recall_at_k(ranked, truth, k): return len(set(ranked[:k]) & set(truth)) / len(truth) if truth else 0.0\n\n# def ndcg_at_k(ranked, truth, k):\n#     dcg = sum(1 / math.log2(idx + 2) for idx, it in enumerate(ranked[:k]) if it in truth)\n#     idcg = sum(1 / math.log2(i + 2) for i in range(min(len(truth), k)))\n#     return dcg / idcg if idcg > 0 else 0.0\n\n# def map_at_k(ranked, truth, k):\n#     hits, s = 0, 0.0\n#     for idx, it in enumerate(ranked[:k]):\n#         if it in truth:\n#             hits += 1\n#             s += hits / (idx + 1)\n#     return s / len(truth) if truth else 0.0\n\n# def mrr_at_k(ranked, truth, k):\n#     for idx, it in enumerate(ranked[:k]):\n#         if it in truth: return 1 / (idx + 1)\n#     return 0.0\n\n# @torch.no_grad()\n# def evaluate_full(model, train_df, test_df, K, loader):\n#     model.eval()\n#     train_map = train_df.groupby(\"uid\")[\"iid\"].apply(set).to_dict()\n#     test_map = test_df.groupby(\"uid\")[\"iid\"].apply(list).to_dict()\n#     P, R, N, AP, MRR = [], [], [], [], []\n#     for u, truth in test_map.items():\n#         if not truth: continue\n#         users = torch.LongTensor([u] * num_items).to(device)\n#         items = torch.arange(num_items).to(device)\n#         ll_item = item_llm_emb.to(device)\n#         ll_user = user_llm_emb[u].unsqueeze(0).repeat(num_items, 1).to(device)\n#         scores = model(users, items, ll_user.unsqueeze(1), ll_item.unsqueeze(1)).view(-1).cpu().numpy()\n#         for it in train_map.get(u, []):\n#             scores[it] = -np.inf\n#         ranked = np.argsort(-scores)\n#         P.append(precision_at_k(ranked, truth, K))\n#         R.append(recall_at_k(ranked, truth, K))\n#         N.append(ndcg_at_k(ranked, truth, K))\n#         AP.append(map_at_k(ranked, truth, K))\n#         MRR.append(mrr_at_k(ranked, truth, K))\n#     return {\n#         \"Precision@K\": np.mean(P),\n#         \"Recall@K\": np.mean(R),\n#         \"NDCG@K\": np.mean(N),\n#         \"MAP@K\": np.mean(AP),\n#         \"MRR@K\": np.mean(MRR),\n#     }\n# === Cell 8: Metric functions (precision@K, recall@K, ndcg@K, MAP@K, MRR@K, HR@K) and evaluate_full ===\nimport math\nimport numpy as np\nimport torch\n\ndef precision_at_k(ranked_list, ground_truth_set, k):\n    \"\"\"\n    Precision@K = |{items in ranked_list[:k]} ∩ ground_truth_set| / k\n    \"\"\"\n    if k == 0:\n        return 0.0\n    num_hit = len(set(ranked_list[:k]) & ground_truth_set)\n    return num_hit / k\n\ndef recall_at_k(ranked_list, ground_truth_set, k):\n    \"\"\"\n    Recall@K = |{items in ranked_list[:k]} ∩ ground_truth_set| / |ground_truth_set|\n    \"\"\"\n    if not ground_truth_set:\n        return 0.0\n    num_hit = len(set(ranked_list[:k]) & ground_truth_set)\n    return num_hit / len(ground_truth_set)\n\ndef ndcg_at_k(ranked_list, ground_truth_set, k):\n    \"\"\"\n    NDCG@K:\n      DCG = sum_{i=0..k-1} (1 / log2(i+2)) if ranked_list[i] in ground_truth_set\n      IDCG = sum_{i=0..min(|ground_truth_set|,k)-1} (1 / log2(i+2))\n      NDCG = DCG / IDCG\n    \"\"\"\n    dcg = 0.0\n    for idx, item in enumerate(ranked_list[:k]):\n        if item in ground_truth_set:\n            dcg += 1.0 / math.log2(idx + 2)\n    ideal_count = min(len(ground_truth_set), k)\n    if ideal_count == 0:\n        return 0.0\n    idcg = sum(1.0 / math.log2(i + 2) for i in range(ideal_count))\n    return dcg / idcg\n\ndef map_at_k(ranked_list, ground_truth_set, k):\n    \"\"\"\n    MAP@K:\n      For each position i in top-K, if ranked_list[i] in ground_truth_set:\n        hits += 1\n        sum_precisions += hits / (i+1)\n      Then divide by |ground_truth_set|.\n    \"\"\"\n    if not ground_truth_set:\n        return 0.0\n    hits = 0\n    sum_precisions = 0.0\n    for idx, item in enumerate(ranked_list[:k]):\n        if item in ground_truth_set:\n            hits += 1\n            sum_precisions += hits / (idx + 1)\n    return sum_precisions / len(ground_truth_set)\n\ndef mrr_at_k(ranked_list, ground_truth_set, k):\n    \"\"\"\n    MRR@K:\n      Return 1 / (rank) of the first relevant item in top-K; else 0.\n    \"\"\"\n    for idx, item in enumerate(ranked_list[:k]):\n        if item in ground_truth_set:\n            return 1.0 / (idx + 1)\n    return 0.0\n\ndef hit_rate_at_k(ranked_list, ground_truth_set, k):\n    \"\"\"\n    HR@K (Hit Rate): 1 if at least one item in top-K is relevant, else 0.\n    \"\"\"\n    return int(bool(set(ranked_list[:k]) & ground_truth_set))\n\n@torch.no_grad()\n\n\ndef evaluate_full(model, train_df, test_df, K, loader, device, num_items, user_llm_emb, item_llm_emb):\n    \"\"\"\n    model: recommendation model returning a score for (user, item)\n    train_df: DataFrame with columns [\"uid\",\"iid\"] (user-item interactions in train)\n    test_df: DataFrame with columns [\"uid\",\"iid\"] (user-item interactions in test; here we treat \n             every interaction as positive for ranking)\n    K: the cutoff for @K\n    loader: not used here (can pass None)\n    device: torch device\n    num_items: total number of items\n    user_llm_emb: tensor of shape (num_users, embed_dim)\n    item_llm_emb: tensor of shape (num_items, embed_dim)\n    \"\"\"\n    model.eval()\n    # Build a map: user -> set of items in train (for filtering)\n    train_map = train_df.groupby(\"uid\")[\"iid\"].apply(set).to_dict()\n\n    # Build ground_truth_map: user -> set of items in test (all are positives)\n    ground_truth_map = test_df.groupby(\"uid\")[\"iid\"].apply(set).to_dict()\n\n    precisions = []\n    recalls = []\n    ndcgs = []\n    hrs = []\n\n    for u, truth_set in ground_truth_map.items():\n        if not truth_set:\n            continue\n\n        # Compute scores for all items for user u\n        users = torch.LongTensor([u] * num_items).to(device)   # shape: (num_items,)\n        items = torch.arange(num_items).to(device)              # shape: (num_items,)\n        ll_user = user_llm_emb[users].to(device)                # shape: (num_items, embed_dim)\n        ll_item = item_llm_emb[items].to(device)                # shape: (num_items, embed_dim)\n        scores = model(users, items, ll_user, ll_item).view(-1).cpu().numpy()\n\n        # Filter out items that appear in train for this user\n        for it in train_map.get(u, set()):\n            scores[it] = -np.inf\n\n        ranked = np.argsort(-scores)  # Indices sorted descending by score\n\n        # Compute metrics @ K\n        precisions.append(precision_at_k(ranked, truth_set, K))\n        recalls.append(recall_at_k(ranked, truth_set, K))\n        ndcgs.append(ndcg_at_k(ranked, truth_set, K))\n        hrs.append(hit_rate_at_k(ranked, truth_set, K))\n\n    return {\n        f\"Precision@{K}\": np.mean(precisions) if precisions else 0.0,\n        f\"Recall@{K}\":    np.mean(recalls)    if recalls    else 0.0,\n        f\"NDCG@{K}\":      np.mean(ndcgs)      if ndcgs      else 0.0,\n        f\"HR@{K}\":        np.mean(hrs)        if hrs        else 0.0,\n    }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T03:56:29.925251Z","iopub.execute_input":"2025-06-05T03:56:29.925914Z","iopub.status.idle":"2025-06-05T03:56:29.941595Z","shell.execute_reply.started":"2025-06-05T03:56:29.925889Z","shell.execute_reply":"2025-06-05T03:56:29.941001Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"pip install -q torch-optimizer\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T03:56:09.410864Z","iopub.status.idle":"2025-06-05T03:56:09.411093Z","shell.execute_reply.started":"2025-06-05T03:56:09.410988Z","shell.execute_reply":"2025-06-05T03:56:09.410999Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === Training loop ===\nbest_ndcg = 0.0\npatience = 15\ncounter = 0\nresults = []\n\nfor ep in range(1, cfg.epochs + 1):\n    model.train()\n    total_loss = 0.0\n\n    for u, i, ll_user, ll_item, r in tqdm(train_loader, desc=f\"Epoch {ep}\"):\n        u = u.to(device)\n        i = i.to(device)\n        ll_user = ll_user.to(device)\n        ll_item = ll_item.to(device)\n        r = r.to(device).float().view(-1)\n\n        optimizer.zero_grad()\n        pred = model(u, i, ll_user, ll_item)\n        if pred.dim() > 1:\n            pred = pred.view(-1)\n        loss = criterion(pred, r)\n        loss.backward()\n\n        # ❌ KHÔNG dùng gradient clipping\n        optimizer.step()\n        total_loss += loss.item() * r.size(0)\n\n    train_loss = total_loss / len(train_loader.dataset)\n\n    # === Evaluation (metrics@K) ===\n    metrics = evaluate_full(\n        model,\n        train_df,\n        test_df,\n        cfg.K,\n        None,\n        device,\n        num_items,\n        user_llm_emb,\n        item_llm_emb\n    )\n\n    metrics[\"Train Loss\"] = train_loss\n    metrics[\"epoch\"] = ep\n    wandb.log(metrics)\n    results.append(metrics)\n\n    print(\n        f\"Epoch {ep} — \"\n        f\"Precision@{cfg.K}={metrics[f'Precision@{cfg.K}']:.4f}  \"\n        f\"Recall@{cfg.K}={metrics[f'Recall@{cfg.K}']:.4f}  \"\n        f\"NDCG@{cfg.K}={metrics[f'NDCG@{cfg.K}']:.4f}  \"\n        f\"HR@{cfg.K}={metrics[f'HR@{cfg.K}']:.4f}\"\n    )\n\n    # === Early stopping based on best NDCG@K ===\n    ndcg = metrics[f\"NDCG@{cfg.K}\"]\n    if ndcg > best_ndcg:\n        best_ndcg = ndcg\n        counter = 0\n        torch.save(model.state_dict(), \"best_model.pt\")\n    else:\n        counter += 1\n        if counter >= patience:\n            print(f\"\\n🛑 Early stopping at epoch {ep} — best NDCG@{cfg.K} = {best_ndcg:.4f}\")\n            break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-05T03:56:33.811637Z","iopub.execute_input":"2025-06-05T03:56:33.812170Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Epoch 1:   0%|          | 0/39 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f48a3138d43446e83891536124e958f"}},"metadata":{}},{"name":"stdout","text":"Epoch 1 — Precision@20=0.0010  Recall@20=0.0039  NDCG@20=0.0020  HR@20=0.0194\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 2:   0%|          | 0/39 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3014f1793f7f434ba3963117073ca9c7"}},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}