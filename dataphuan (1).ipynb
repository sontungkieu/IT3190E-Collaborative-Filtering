{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11953048,"sourceType":"datasetVersion","datasetId":7514853},{"sourceId":12053537,"sourceType":"datasetVersion","datasetId":7474913}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q sentence-transformers wandb","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-03T15:48:54.532156Z","iopub.execute_input":"2025-06-03T15:48:54.532966Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 1: Seed v√† imports\nimport os, random, math\nimport numpy as np\nimport pandas as pd\nfrom tqdm.auto import tqdm\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\n\nfrom sentence_transformers import SentenceTransformer\nimport wandb\n\n# --- Seed ƒë·ªÉ t√°i l·∫≠p ---\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed_all(SEED)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark     = False\n\n# Device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device={device}, torch={torch.__version__}\")\n\n# W&B init\nName = \"NgocMinh\"\nModel_name = \"NeuFM + LLM (MSE)\"\nVersion = \"1.0.0\"\n\nwandb.login(key=\"62e3cf4c2815c959ed2609de1d55fa0504818c4a\")\n\n# 1.1. Kh·ªüi t·∫°o W&B run\nwandb.init(\n    entity=\"IT3190E-20242-MachinLearning\",\n    project=\"cf-electronics\",\n    name=\"HybridNeuMF_MSE\",\n    config={\n        \"mf_dim\": 32,\n        \"mlp_layers\": [64, 32, 16, 8],\n        \"llm_model\": \"all-MiniLM-L6-v2\",  # üü¢ C√ì d√≤ng n√†y\n        \"llm_dim\": 384,\n        \"batch_size\": 1024,\n        \"num_neg\": 4,\n        \"lr\": 1e-3,\n        \"weight_decay\": 1e-5,\n        \"epochs\": 10,\n        \"K\": 10\n    }\n)\ncfg = wandb.config\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T09:29:17.536044Z","iopub.execute_input":"2025-06-04T09:29:17.536579Z","iopub.status.idle":"2025-06-04T09:29:30.172399Z","shell.execute_reply.started":"2025-06-04T09:29:17.536555Z","shell.execute_reply":"2025-06-04T09:29:30.171660Z"}},"outputs":[{"name":"stdout","text":"Using device=cuda, torch=2.6.0+cu124\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkieusontung8\u001b[0m (\u001b[33mkieusontung8-hanoi-university-of-science-and-technology\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250604_092923-og2wvhie</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/IT3190E-20242-MachinLearning/cf-electronics/runs/og2wvhie' target=\"_blank\">HybridNeuMF_MSE</a></strong> to <a href='https://wandb.ai/IT3190E-20242-MachinLearning/cf-electronics' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/IT3190E-20242-MachinLearning/cf-electronics' target=\"_blank\">https://wandb.ai/IT3190E-20242-MachinLearning/cf-electronics</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/IT3190E-20242-MachinLearning/cf-electronics/runs/og2wvhie' target=\"_blank\">https://wandb.ai/IT3190E-20242-MachinLearning/cf-electronics/runs/og2wvhie</a>"},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"# Cell 2: Load ratings, movies, users\nratings = pd.read_csv(\"/kaggle/input/movie-lens-1m/final_ratings.csv\")    # user_id,item_id,rating,timestamp\nmovies  = pd.read_csv(\"/kaggle/input/movie-lens-1m/final_movies.csv\")     # item_id,title,genre,avg_rating,num_ratings,description\nusers   = pd.read_csv(\"/kaggle/input/movie-lens-1m/users.csv\")      # user_id,gender,age,occupation,zip_code\n\n# 1) Label-encode user_id, item_id\nuser_enc = LabelEncoder(); ratings[\"uid\"] = user_enc.fit_transform(ratings.user_id)\nitem_enc = LabelEncoder(); ratings[\"iid\"] = item_enc.fit_transform(ratings.item_id)\nnum_users = ratings.uid.nunique()\nnum_items = ratings.iid.nunique()\n\n# 2) Encode user metadata:\n#   - gender: M‚Üí1, F‚Üí0\n#   - age: normalized [0,1]\n#   - occupation: one-hot\nusers[\"gender_bin\"] = (users.gender == \"M\").astype(int)\nmax_age = users.age.max()\nusers[\"age_norm\"] = users.age / max_age\nocc_ohe = pd.get_dummies(users.occupation, prefix=\"occ\")\nusers = pd.concat([users, occ_ohe], axis=1)\n# Build a matrix [num_users √ó meta_dim]\nuser_meta = users.sort_values(\"user_id\").reset_index(drop=True)\nmeta_cols = [\"gender_bin\", \"age_norm\"] + list(occ_ohe.columns)\nuser_meta_matrix = torch.tensor(\n    user_meta[meta_cols].values.astype(np.float32),\n    device=device\n)  # shape: [num_users, meta_dim]\n\nprint(f\"Users={num_users}, Items={num_items}, meta_dim={user_meta_matrix.shape[1]}\")\n\n# 3) Train/test split theo timestamp (80/20)\nratings = ratings.sort_values(\"timestamp\")\ntrain_df, test_df = train_test_split(ratings, test_size=0.2, shuffle=False)\nprint(f\"Train interactions={len(train_df)}, Test interactions={len(test_df)}\")\n\n# 4) Chu·∫©n b·ªã text ƒë·ªÉ encode LLM\nmovies[\"text_input\"] = (\n    movies.title.fillna(\"\") + \" \" +\n    movies.text.fillna(\"\") \n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T09:29:43.535594Z","iopub.execute_input":"2025-06-04T09:29:43.536149Z","iopub.status.idle":"2025-06-04T09:29:44.861385Z","shell.execute_reply.started":"2025-06-04T09:29:43.536125Z","shell.execute_reply":"2025-06-04T09:29:44.860618Z"}},"outputs":[{"name":"stdout","text":"Users=6040, Items=3706, meta_dim=23\nTrain interactions=800167, Test interactions=200042\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# === Cell Save 2b: L∆∞u t·∫°m d·ªØ li·ªáu ƒë√£ ti·ªÅn x·ª≠ l√Ω (sau Cell 2) ===\nimport pandas as pd\nimport torch\nimport gc\n\n# Gi·∫£ ƒë·ªãnh c√°c bi·∫øn t·ª´ Cell 2: train_df, test_df, movies, users, user_meta_matrix\ntrain_df.to_parquet(\"train_df.parquet\")\ntest_df.to_parquet(\"test_df.parquet\")\nmovies.to_parquet(\"movies.parquet\")\nusers.to_parquet(\"users.parquet\")\ntorch.save(user_meta_matrix.cpu(), \"user_meta_matrix.pt\")\n\n# Gi·∫£i ph√≥ng b·ªô nh·ªõ nh·ªØng bi·∫øn l·ªõn\ndel train_df, test_df, movies, users, user_meta_matrix\ngc.collect()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T08:31:59.007253Z","iopub.execute_input":"2025-06-04T08:31:59.007869Z","iopub.status.idle":"2025-06-04T08:31:59.749791Z","shell.execute_reply.started":"2025-06-04T08:31:59.007843Z","shell.execute_reply":"2025-06-04T08:31:59.749071Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === Cell Load 3a: T·∫£i l·∫°i d·ªØ li·ªáu tr∆∞·ªõc khi sinh LLM embedding (Cell 3) ===\nimport pandas as pd\nimport torch\n\n# Load l·∫°i DataFrame v√† tensor metadata\ntrain_df = pd.read_parquet(\"train_df.parquet\")\ntest_df  = pd.read_parquet(\"test_df.parquet\")\nmovies   = pd.read_parquet(\"movies.parquet\")\nusers    = pd.read_parquet(\"users.parquet\")\n\n# Load l·∫°i user metadata matrix\nuser_meta_matrix = torch.load(\"user_meta_matrix.pt\", map_location=device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T08:32:01.774017Z","iopub.execute_input":"2025-06-04T08:32:01.774543Z","iopub.status.idle":"2025-06-04T08:32:01.894033Z","shell.execute_reply.started":"2025-06-04T08:32:01.774492Z","shell.execute_reply":"2025-06-04T08:32:01.893498Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n# # üõ† ƒê·ªìng b·ªô th·ª© t·ª± item trong movies v·ªõi item_enc.classes_\n# # item_enc.classes_ ch·ª©a item_id (g·ªëc) ƒë√£ ƒë∆∞·ª£c m√£ h√≥a th√†nh 0...num_items-1\n# used_item_ids = item_enc.classes_\n\n# # S·∫Øp x·∫øp l·∫°i movies theo th·ª© t·ª± item_id ƒë√£ m√£ h√≥a\n# movies = movies.set_index(\"item_id\").loc[used_item_ids].reset_index()\n# assert len(movies) == num_items  # ƒë·∫£m b·∫£o ƒë√∫ng k√≠ch th∆∞·ªõc\n\n# # Chu·∫©n b·ªã text input ƒë·ªÉ encode\n# item_texts = (\n#     movies[\"title\"].fillna(\"\") + \" \" +\n#     movies[\"genre\"].fillna(\"\") + \" \" +\n#     movies[\"description\"].fillna(\"\")\n# ).tolist()\n\n# # Encode b·∫±ng LLM\n# llm = SentenceTransformer(cfg.llm_model, device=device)\n# item_llm_emb = llm.encode(\n#     item_texts,\n#     batch_size=64,\n#     show_progress_bar=True,\n#     convert_to_numpy=True,\n#     device=device\n# )\n\n# # Ki·ªÉm tra v√† convert sang tensor\n# assert item_llm_emb.shape == (num_items, cfg.llm_dim), \"S·ªë l∆∞·ª£ng ho·∫∑c chi·ªÅu embedding kh√¥ng kh·ªõp\"\n# item_llm_emb = torch.tensor(item_llm_emb, dtype=torch.float32, device=device)\n# üõ† ƒê·ªìng b·ªô th·ª© t·ª± item trong movies v·ªõi item_enc.classes_\n# item_enc.classes_ ch·ª©a item_id (g·ªëc) ƒë√£ ƒë∆∞·ª£c m√£ h√≥a th√†nh 0...num_items-1\nused_item_ids = item_enc.classes_\n\n# S·∫Øp x·∫øp l·∫°i movies theo th·ª© t·ª± item_id ƒë√£ m√£ h√≥a\nmovies = movies.set_index(\"item_id\").loc[used_item_ids].reset_index()\nassert len(movies) == num_items  # ƒë·∫£m b·∫£o ƒë√∫ng k√≠ch th∆∞·ªõc\n\n# Chu·∫©n b·ªã text input ƒë·ªÉ encode\nitem_texts = (\n    movies[\"title\"].fillna(\"\") + \" \" +\n    movies[\"text\"].fillna(\"\")\n).tolist()\n\n# Encode b·∫±ng LLM\nllm = SentenceTransformer(cfg.llm_model, device=device)\nitem_llm_emb = llm.encode(\n    item_texts,\n    batch_size=64,\n    show_progress_bar=True,\n    convert_to_numpy=True,\n    device=device\n)\n\n# Ki·ªÉm tra v√† convert sang tensor\nassert item_llm_emb.shape == (num_items, cfg.llm_dim), \"S·ªë l∆∞·ª£ng ho·∫∑c chi·ªÅu embedding kh√¥ng kh·ªõp\"\nitem_llm_emb = torch.tensor(item_llm_emb, dtype=torch.float32, device=device)\n\n# --- Ph·∫ßn m·ªõi: L∆∞u k√®m item_ids v·ªõi embeddings ---\n# Chuy·ªÉn embeddings v·ªÅ CPU tr∆∞·ªõc khi save (kh√¥ng b·∫Øt bu·ªôc, nh∆∞ng th√¥ng th∆∞·ªùng d·ªÖ load h∆°n)\nitem_llm_emb_cpu = item_llm_emb.detach().cpu()\n\n# T·∫°o dict ch·ª©a song song\noutput = {\n    \"item_ids\": used_item_ids,       # danh s√°ch item_id ·ª©ng v·ªõi m·ªói vector\n    \"embeddings\": item_llm_emb_cpu   # tensor shape = (num_items, llm_dim)\n}\n\n# L∆∞u ra file m·ªõi (c√≥ th·ªÉ ƒë·ªïi t√™n t√πy √Ω)\ntorch.save(output, \"item_llm_emb_with_ids.pt\")\nprint(output)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-04T09:29:49.322337Z","iopub.execute_input":"2025-06-04T09:29:49.322614Z","iopub.status.idle":"2025-06-04T09:30:02.969818Z","shell.execute_reply.started":"2025-06-04T09:29:49.322592Z","shell.execute_reply":"2025-06-04T09:30:02.969158Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c5f11cfad1e4e24b91796c34be594e8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"77ac53b0651b4cea9467e803e01abfcb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/10.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9265687838e748ecb54e0c7e2f7a175b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa6e94ac93d5422f9a845f9b750fd6da"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0de67e159dce45a18c775c5d2744cd34"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"779cf522357c4befa7f66478fb5da263"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bee6c39ea10244a2972607ab7d2b1913"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"634e409ca3384f7d8acde9a698965cfb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2c0e0582fd454564a174532ce2c13bb9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1058bae23f964817a1568f42924e8598"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6fa3ee5647de4b698cc99079a167e787"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/58 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac8c5f4fa9ce42469cbff93602915be1"}},"metadata":{}},{"name":"stdout","text":"{'item_ids': array([   0,    1,    2, ..., 3880, 3881, 3882]), 'embeddings': tensor([[-0.0081, -0.0436,  0.1110,  ...,  0.0023,  0.0441,  0.0532],\n        [-0.0116,  0.0919,  0.0375,  ...,  0.0328, -0.1164, -0.0062],\n        [-0.0728, -0.0422,  0.0172,  ..., -0.0119,  0.0216, -0.0244],\n        ...,\n        [-0.0622, -0.0325, -0.0619,  ...,  0.0439, -0.0169, -0.0491],\n        [-0.0369,  0.0104, -0.0009,  ...,  0.1165, -0.0494, -0.0403],\n        [-0.0522, -0.0572, -0.0330,  ...,  0.0047,  0.0041, -0.0092]])}\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# === Cell Load 4a: T·∫£i l·∫°i LLM embedding tr∆∞·ªõc khi d√πng trong Dataset (Cell 4) ===\nimport torch\n\n# Load l·∫°i embedding v√†o ƒë√∫ng device\nitem_llm_emb = torch.load(\"item_llm_emb_with_ids.pt\", map_location=device)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 4: Dataset with negative sampling + metadata + llm_emb\n# class HybridDataset(Dataset):\n#     def __init__(self, df, num_items, user_meta, llm_emb, num_neg=4):\n#         self.df        = df.reset_index(drop=True)\n#         self.num_items = num_items\n#         self.user_meta = user_meta\n#         self.llm_emb   = llm_emb\n#         self.num_neg   = num_neg\n#         self.pos_set   = set(zip(df.uid, df.iid))\n#         self.users, self.items, self.labels = self._prepare()\n\n#     def _prepare(self):\n#         U,I,L = [],[],[]\n#         for u,i in zip(self.df.uid, self.df.iid):\n#             U.append(u); I.append(i); L.append(1.0)\n#             for _ in range(self.num_neg):\n#                 neg = np.random.randint(self.num_items)\n#                 while (u, neg) in self.pos_set:\n#                     neg = np.random.randint(self.num_items)\n#                 U.append(u); I.append(neg); L.append(0.0)\n#         return U,I,L\n\n#     def __len__(self): return len(self.labels)\n\n#     def __getitem__(self, idx):\n#         u = torch.LongTensor([self.users[idx]])\n#         i = torch.LongTensor([self.items[idx]])\n#         l = torch.FloatTensor([self.labels[idx]])\n#         meta = self.user_meta[self.users[idx]].unsqueeze(0)  # [1,meta_dim]\n#         ll   = self.llm_emb[self.items[idx]].unsqueeze(0)   # [1,llm_dim]\n#         return u, i, meta, ll, l\n\n# train_ds = HybridDataset(train_df, num_items, user_meta_matrix, item_llm_emb, cfg.num_neg)\n# test_ds  = HybridDataset(test_df,  num_items, user_meta_matrix, item_llm_emb, 0)\n\n# train_loader = DataLoader(train_ds,\n#                           batch_size=cfg.batch_size,\n#                           shuffle=True,  num_workers=4)\n# test_loader  = DataLoader(test_ds,\n#                           batch_size=cfg.batch_size,\n#                           shuffle=False, num_workers=4)\n# === Cell 4: Dataset d√πng rating th·∫≠t (explicit feedback) ===\n# === Cell 4: Dataset d√πng rating th·∫≠t (explicit feedback) ===\nclass RatingDataset(Dataset):\n    def __init__(self, df, user_meta, item_llm_emb):\n        self.df = df.reset_index(drop=True)\n        self.users = df[\"uid\"].tolist()\n        self.items = df[\"iid\"].tolist()\n        self.ratings = df[\"rating\"].tolist()\n        self.user_meta = user_meta.cpu()       # ƒë·ªÉ tr√°nh l·ªói multiprocess\n        self.item_llm = item_llm_emb.cpu()\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        u = torch.LongTensor([self.users[idx]])\n        i = torch.LongTensor([self.items[idx]])\n        r = torch.FloatTensor([self.ratings[idx]])   # d√πng rating th·∫≠t\n\n        meta = self.user_meta[self.users[idx]].unsqueeze(0)\n        ll   = self.item_llm[self.items[idx]].unsqueeze(0)\n        return u, i, meta, ll, r\n\n# T·∫°o Dataset + DataLoader kh√¥ng c·∫ßn negative sampling\ntrain_ds = RatingDataset(train_df, user_meta_matrix, item_llm_emb)\ntest_ds  = RatingDataset(test_df,  user_meta_matrix, item_llm_emb)\n\ntrain_loader = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True,  num_workers=0)\ntest_loader  = DataLoader(test_ds, batch_size=cfg.batch_size, shuffle=False, num_workers=0)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === Cell 5: M√¥ h√¨nh NeuMF + Metadata + LLM cho Rating Prediction ===\nclass HybridNeuMF(nn.Module):\n    def __init__(self, n_users, n_items, mf_dim, mlp_layers, meta_dim, llm_dim):\n        super().__init__()\n        self.user_mf = nn.Embedding(n_users, mf_dim)\n        self.item_mf = nn.Embedding(n_items, mf_dim)\n\n        mlp_input = mlp_layers[0]\n        self.user_mlp = nn.Embedding(n_users, mlp_input // 2)\n        self.item_mlp = nn.Embedding(n_items, mlp_input // 2)\n\n        mlp_blocks = []\n        for in_d, out_d in zip(mlp_layers[:-1], mlp_layers[1:]):\n            mlp_blocks += [nn.Dropout(0.2), nn.Linear(in_d, out_d), nn.ReLU()]\n        self.mlp = nn.Sequential(*mlp_blocks)\n\n        fusion_dim = mf_dim + mlp_layers[-1] + meta_dim + llm_dim\n        self.predict = nn.Sequential(\n            nn.Linear(fusion_dim, 64),\n            nn.ReLU(),\n            nn.Linear(64, 1)      # ‚ùå kh√¥ng d√πng sigmoid\n        )\n\n    def forward(self, u, i, meta, ll):\n        mu = self.user_mf(u).squeeze(1)\n        mi = self.item_mf(i).squeeze(1)\n        mf_vec = mu * mi\n\n        xu = self.user_mlp(u).squeeze(1)\n        xi = self.item_mlp(i).squeeze(1)\n        mlp_vec = self.mlp(torch.cat([xu, xi], dim=1))\n\n        x = torch.cat([mf_vec, mlp_vec, meta.squeeze(1), ll.squeeze(1)], dim=1)\n        return self.predict(x)\n\n# Kh·ªüi t·∫°o model v√† loss\nmodel = HybridNeuMF(\n    num_users, num_items,\n    mf_dim=cfg.mf_dim,\n    mlp_layers=cfg.mlp_layers,\n    meta_dim=user_meta_matrix.shape[1],\n    llm_dim=cfg.llm_dim\n).to(device)\n\ncriterion = nn.MSELoss()   # ‚úÖ D√πng MSELoss ƒë·ªÉ d·ª± ƒëo√°n rating\noptimizer = torch.optim.Adam(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n\nwandb.watch(model, log=\"all\", log_freq=100)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import math\n# import numpy as np\n# import torch\n\n# # --- Top-K Ranking Metrics ---\n# def precision_at_k(ranked, truth, k):\n#     return len(set(ranked[:k]) & set(truth)) / k\n\n# def recall_at_k(ranked, truth, k):\n#     return len(set(ranked[:k]) & set(truth)) / len(truth) if len(truth) > 0 else 0.0\n\n# def ndcg_at_k(ranked, truth, k):\n#     dcg = sum(1 / math.log2(idx + 2) for idx, item in enumerate(ranked[:k]) if item in truth)\n#     idcg = sum(1 / math.log2(i + 2) for i in range(min(len(truth), k)))\n#     return dcg / idcg if idcg > 0 else 0.0\n\n# def map_at_k(ranked, truth, k):\n#     hits, s = 0, 0.0\n#     for idx, item in enumerate(ranked[:k]):\n#         if item in truth:\n#             hits += 1\n#             s += hits / (idx + 1)\n#     return s / len(truth) if len(truth) > 0 else 0.0\n\n# def mrr_at_k(ranked, truth, k):\n#     for idx, item in enumerate(ranked[:k]):\n#         if item in truth:\n#             return 1 / (idx + 1)\n#     return 0.0\n\n# # --- RMSE cho rating th·∫≠t (explicit feedback) ---\n# def rmse_real(model, loader):\n#     model.eval()\n#     se, n = 0.0, 0\n#     with torch.no_grad():\n#         for u, i, meta, ll, r in loader:\n#             u, i = u.to(device), i.to(device)\n#             meta, ll, r = meta.to(device), ll.to(device), r.to(device)\n#             pred = model(u, i, meta, ll).view(-1)\n#             r = r.view(-1)\n#             se += ((pred - r) ** 2).sum().item()\n#             n += r.size(0)\n#     return math.sqrt(se / n)\n\n# # --- H√†m ƒë√°nh gi√° to√†n di·ªán ---\n# @torch.no_grad()\n# def evaluate_full(model, train_df, test_df, K, test_loader):\n#     model.eval()\n\n#     # T·∫≠p item ƒë√£ th·∫•y (ƒë·ªÉ mask)\n#     train_map = train_df.groupby(\"uid\")[\"iid\"].apply(set).to_dict()\n#     test_map  = test_df.groupby(\"uid\")[\"iid\"].apply(list).to_dict()\n\n#     P_list, R_list, N_list, MAP_list, MRR_list = [], [], [], [], []\n\n#     for u, truth in test_map.items():\n#         if len(truth) == 0:\n#             continue  # b·ªè qua user kh√¥ng c√≥ ground truth\n\n#         users = torch.LongTensor([u] * num_items).to(device)\n#         items = torch.arange(num_items).to(device)\n#         metas = user_meta_matrix[u].unsqueeze(0).repeat(num_items, 1).to(device)\n#         lls   = item_llm_emb.to(device)\n\n#         scores = model(users, items, metas.unsqueeze(1), lls.unsqueeze(1)).view(-1).cpu().numpy()\n\n#         # Mask item ƒë√£ t·ª´ng th·∫•y trong train\n#         for it in train_map.get(u, []):\n#             scores[it] = -np.inf\n\n#         ranked = np.argsort(-scores)\n\n#         P_list.append(precision_at_k(ranked, truth, K))\n#         R_list.append(recall_at_k(ranked, truth, K))\n#         N_list.append(ndcg_at_k(ranked, truth, K))\n#         MAP_list.append(map_at_k(ranked, truth, K))\n#         MRR_list.append(mrr_at_k(ranked, truth, K))\n\n#     return {\n#         \"Precision@K\": np.mean(P_list),\n#         \"Recall@K\":    np.mean(R_list),\n#         \"NDCG@K\":      np.mean(N_list),\n#         \"MAP@K\":       np.mean(MAP_list),\n#         \"MRR@K\":       np.mean(MRR_list),\n#         \"RMSE\":        rmse_real(model, test_loader)\n#     }\n# === Cell 8: Metric functions (precision@K, recall@K, ndcg@K, HR@K) and evaluate_full ===\nimport math\nimport numpy as np\nimport torch\n\n# --- Top-K Ranking Metrics ---\ndef precision_at_k(ranked, truth, k):\n    \"\"\"\n    Precision@K = |{items in ranked[:k]} ‚à© truth| / k\n    \"\"\"\n    if k == 0:\n        return 0.0\n    return len(set(ranked[:k]) & truth) / k\n\ndef recall_at_k(ranked, truth, k):\n    \"\"\"\n    Recall@K = |{items in ranked[:k]} ‚à© truth| / |truth|\n    \"\"\"\n    if len(truth) == 0:\n        return 0.0\n    return len(set(ranked[:k]) & truth) / len(truth)\n\ndef ndcg_at_k(ranked, truth, k):\n    \"\"\"\n    NDCG@K:\n      DCG = sum_{i=0..k-1} (1 / log2(i+2)) if ranked[i] in truth\n      IDCG = sum_{i=0..min(|truth|,k)-1} (1 / log2(i+2))\n    \"\"\"\n    dcg = 0.0\n    for idx, item in enumerate(ranked[:k]):\n        if item in truth:\n            dcg += 1.0 / math.log2(idx + 2)\n    ideal_count = min(len(truth), k)\n    if ideal_count == 0:\n        return 0.0\n    idcg = sum(1.0 / math.log2(i + 2) for i in range(ideal_count))\n    return dcg / idcg\n\ndef hit_rate_at_k(ranked, truth, k):\n    \"\"\"\n    HR@K = 1 if at least one item in ranked[:k] is in truth; else 0.\n    \"\"\"\n    return int(bool(set(ranked[:k]) & truth))\n\n# --- RMSE cho rating th·∫≠t (explicit feedback) ---\ndef rmse_real(model, loader):\n    model.eval()\n    se, n = 0.0, 0\n    with torch.no_grad():\n        for u, i, meta, ll, r in loader:\n            u, i = u.to(device), i.to(device)\n            meta, ll, r = meta.to(device), ll.to(device), r.to(device)\n            pred = model(u, i, meta, ll).view(-1)\n            r = r.view(-1)\n            se += ((pred - r) ** 2).sum().item()\n            n += r.size(0)\n    return math.sqrt(se / n)\n\n@torch.no_grad()\ndef evaluate_full(model, train_df, test_df, K, test_loader):\n    \"\"\"\n    model: recommendation model returning a score for (u,i,meta,ll)\n    train_df: DataFrame with [\"uid\",\"iid\"] from train set\n    test_df: DataFrame with [\"uid\",\"iid\",\"rating\"] from test set\n             We treat rating >= 3.5 as ‚Äúrelevant.‚Äù\n    K: cutoff for @K\n    test_loader: DataLoader for test set (used only to compute RMSE via rmse_real)\n    \"\"\"\n    model.eval()\n\n    # 1) Map user -> set of items seen in train (ƒë·ªÉ mask)\n    train_map = train_df.groupby(\"uid\")[\"iid\"].apply(set).to_dict()\n\n    # 2) Map user -> set of items ‚Äúrelevant‚Äù (rating >= 3.5)\n    test_pos = test_df[test_df[\"rating\"] >= 3.5]\n    ground_truth_map = test_pos.groupby(\"uid\")[\"iid\"].apply(set).to_dict()\n\n    P_list, R_list, N_list, HR_list = [], [], [], []\n\n    for u, truth in ground_truth_map.items():\n        if len(truth) == 0:\n            continue  # b·ªè qua user kh√¥ng c√≥ item relevant\n\n        # T√≠nh score cho t·∫•t c·∫£ items\n        users = torch.LongTensor([u] * num_items).to(device)       # shape = (num_items,)\n        items = torch.arange(num_items).to(device)                  # shape = (num_items,)\n        metas = user_meta_matrix[u].unsqueeze(0).repeat(num_items, 1).to(device)  # (num_items, meta_dim)\n        lls   = item_llm_emb.to(device)                                       # (num_items, llm_dim)\n\n        scores = model(users, items, metas.unsqueeze(1), lls.unsqueeze(1)).view(-1).cpu().numpy()\n\n        # Mask h·∫øt c√°c item ƒë√£ xu·∫•t hi·ªán trong train\n        for it in train_map.get(u, set()):\n            scores[it] = -np.inf\n\n        ranked = np.argsort(-scores)  # indices s·∫Øp x·∫øp gi·∫£m d·∫ßn theo score\n\n        P_list.append( precision_at_k(ranked, truth, K) )\n        R_list.append( recall_at_k(ranked, truth, K) )\n        N_list.append( ndcg_at_k(ranked, truth, K) )\n        HR_list.append( hit_rate_at_k(ranked, truth, K) )\n\n    return {\n        f\"Precision@{K}\": np.mean(P_list) if P_list else 0.0,\n        f\"Recall@{K}\":    np.mean(R_list) if R_list else 0.0,\n        f\"NDCG@{K}\":      np.mean(N_list) if N_list else 0.0,\n        f\"HR@{K}\":        np.mean(HR_list) if HR_list else 0.0,\n        \"RMSE\":          rmse_real(model, test_loader)\n    }\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === Cell 9: Training loop with early stopping ===\nfrom tqdm import tqdm\n\nbest_ndcg = 0.0\npatience = 3\ncounter = 0\nresults = []\n\nfor ep in range(1, cfg.epochs + 1):\n    model.train()\n    total_loss = 0.0\n\n    for u, i, meta, ll, r in tqdm(train_loader, desc=f\"Epoch {ep}\"):\n        u, i = u.to(device), i.to(device)\n        meta, ll, r = meta.to(device), ll.to(device), r.to(device)\n\n        optimizer.zero_grad()\n        pred = model(u, i, meta, ll).view(-1)\n        loss = criterion(pred, r.view(-1))\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item() * r.size(0)\n\n    train_loss = total_loss / len(train_ds)\n\n    # G·ªçi evaluate_full ƒë·ªÉ t√≠nh Precision@K, Recall@K, NDCG@K, HR@K, RMSE\n    metrics = evaluate_full(model, train_df, test_df, cfg.K, test_loader)\n    metrics[\"Train Loss\"] = train_loss\n    metrics[\"epoch\"] = ep\n\n    wandb.log(metrics)\n    results.append(metrics)\n\n    # In ra tr√™n console: Precision@K, Recall@K, NDCG@K, HR@K, RMSE\n    print(\n        f\"Epoch {ep} ‚Äî \"\n        f\"Precision@{cfg.K}={metrics[f'Precision@{cfg.K}']:.4f}  \"\n        f\"Recall@{cfg.K}={metrics[f'Recall@{cfg.K}']:.4f}  \"\n        f\"NDCG@{cfg.K}={metrics[f'NDCG@{cfg.K}']:.4f}  \"\n        f\"HR@{cfg.K}={metrics[f'HR@{cfg.K}']:.4f}  \"\n        f\"RMSE={metrics['RMSE']:.4f}\"\n    )\n\n    # Early stopping d·ª±a tr√™n NDCG@K\n    ndcg_val = metrics[f\"NDCG@{cfg.K}\"]\n    if ndcg_val > best_ndcg:\n        best_ndcg = ndcg_val\n        counter = 0\n        torch.save(model.state_dict(), \"best_model.pt\")\n    else:\n        counter += 1\n        if counter >= patience:\n            print(f\"\\nüõë Early stopping at epoch {ep} ‚Äî best NDCG@{cfg.K} = {best_ndcg:.4f}\")\n            break\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==== Cell: Inspect one batch ====\nbatch = next(iter(train_loader))\nprint(f\"Batch length: {len(batch)}\")\nfor i, t in enumerate(batch):\n    try:\n        print(f\"  idx {i}: shape={tuple(t.shape)}, dtype={t.dtype}\")\n    except AttributeError:          # ch·∫≥ng h·∫°n n·∫øu ph·∫ßn t·ª≠ l√† int/float\n        print(f\"  idx {i}: value={t}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==== Cell: Compute HR@K with metadata & LLM input ====\nK = 10\nn_neg = 100\nNUM_ITEMS = num_items  # t·ªïng s·ªë item sau khi encode\n\ndef compute_hit_rate(model, data_loader, K=10, n_neg=100,\n                     device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n    \"\"\"\n    T√≠nh HR@K cho model HybridNeuMF(user, item, meta, ll)\n    D·ª±a tr√™n batch c√≥ 5 tr∆∞·ªùng:\n      [0] user_id (B,1), [1] pos_item (B,1), [2] user_meta (B,1,23), [3] item_llm (B,1,384), [4] rating\n    \"\"\"\n    model.eval()\n    hits, total = 0, 0\n\n    with torch.no_grad():\n        for batch in data_loader:\n            user      = batch[0].squeeze(1).to(device)     # (B,)\n            pos_item  = batch[1].squeeze(1).to(device)     # (B,)\n            user_meta = batch[2].squeeze(1).to(device)     # (B, 23)\n            item_llm  = batch[3].squeeze(1).to(device)     # (B, 384)\n\n            B = user.size(0)\n\n            # Sample negatives\n            neg_items = torch.randint(\n                0, NUM_ITEMS, size=(B, n_neg), device=device\n            )\n            dup_mask = (neg_items == pos_item.unsqueeze(1))\n            while dup_mask.any():\n                neg_items[dup_mask] = torch.randint(\n                    0, NUM_ITEMS, size=(dup_mask.sum(),), device=device\n                )\n                dup_mask = (neg_items == pos_item.unsqueeze(1))\n\n            # Candidates = [pos_item] + neg_items\n            candidates = torch.cat(\n                [pos_item.unsqueeze(1), neg_items], dim=1\n            )  # (B, n_neg + 1)\n\n            users_flat = user.unsqueeze(1).expand_as(candidates).reshape(-1)\n            items_flat = candidates.reshape(-1)\n\n            # M·ªü r·ªông user_meta & item_llm t∆∞∆°ng ·ª©ng (broadcast ƒë√∫ng chi·ªÅu)\n            meta_expanded = user_meta.unsqueeze(1).expand(B, n_neg + 1, -1).reshape(-1, user_meta.shape[-1])\n            llm_expanded  = item_llm.unsqueeze(1).expand(B, n_neg + 1, -1).reshape(-1, item_llm.shape[-1])\n\n            # G·ªçi forward\n            scores = model(users_flat, items_flat, meta_expanded, llm_expanded)\n            scores = scores.reshape(candidates.shape)\n\n            _, topk_idx = scores.topk(K, dim=1)\n            hits += (topk_idx == 0).any(dim=1).sum().item()\n            total += B\n\n    return hits / total if total else 0.0\n\n# üîç ƒê√°nh gi√° HR@10\nhrK_train = compute_hit_rate(model, train_loader, K=K, n_neg=n_neg)\nhrK_test  = compute_hit_rate(model, test_loader,  K=K, n_neg=n_neg)\n\nprint(f\"‚úÖ HR@{K} on train set: {hrK_train:.4f}\")\nprint(f\"‚úÖ HR@{K} on test  set: {hrK_test :.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 9: Show & save\ndf_res = pd.DataFrame(results).set_index(\"epoch\")\ndisplay(df_res)\ntorch.save(model.state_dict(), \"/kaggle/working/hybrid_neumf_llm.pth\")\ndf_res.to_csv(\"/kaggle/working/hybrid_neumf_llm_metrics.csv\")\nprint(\"‚úÖ Saved model & metrics.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}