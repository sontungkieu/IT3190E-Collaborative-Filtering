{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11964209,"sourceType":"datasetVersion","datasetId":7523175},{"sourceId":11969531,"sourceType":"datasetVersion","datasetId":7516351}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Cell 1: Cài đặt (chạy 1 lần)\n!pip install -q sentence-transformers wandb","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-27T13:15:16.168324Z","iopub.execute_input":"2025-05-27T13:15:16.168670Z","iopub.status.idle":"2025-05-27T13:16:52.323049Z","shell.execute_reply.started":"2025-05-27T13:15:16.168643Z","shell.execute_reply":"2025-05-27T13:16:52.321194Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 2: Imports, seed, device & W&B init\nimport os, random, math\nimport numpy as np\nimport pandas as pd\nfrom tqdm.auto import tqdm\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sentence_transformers import SentenceTransformer\nimport wandb\n\n# --- Seed để tái lập ---\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed_all(SEED)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark     = False\n\n# Thiết lập tokenizer parallelism để tránh warning\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\n# Device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device={device}\")\n\nwandb.finish()  \n# W&B init\nName = \"NgocMinh\"\nModel_name = \"LightFM\"\nVersion = \"1.0.0\"\n\nwandb.login(key=\"62e3cf4c2815c959ed2609de1d55fa0504818c4a\")\n\n# 1.1. Khởi tạo W&B run\nwandb.init(\n    project=\"hybrid-neumf-llm-newds\",\n    name=\"HybridNeuMF_NewDS\",\n    config={\n        \"mf_dim\": 32,\n        \"mlp_layers\": [64, 32, 16, 8],\n        \"llm_model\": \"all-MiniLM-L6-v2\",\n        \"llm_dim\": 384,\n        \"batch_size\": 1024,\n        \"lr\": 1e-3,\n        \"weight_decay\": 1e-4,\n        \"epochs\": 10,\n        \"K\": 10\n    }\n)\ncfg = wandb.config\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T14:50:13.685780Z","iopub.execute_input":"2025-05-27T14:50:13.686367Z","iopub.status.idle":"2025-05-27T14:50:27.834776Z","shell.execute_reply.started":"2025-05-27T14:50:13.686341Z","shell.execute_reply":"2025-05-27T14:50:27.834034Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 3: Load & encode IDs\ntrain_df = pd.read_csv(\"/kaggle/input/databang/train_ratings.csv\")   # user_id,item_id,rating,...\ntest_df  = pd.read_csv(\"/kaggle/input/databang/test_ratings.csv\")\nmeta_df  = pd.read_csv(\"/kaggle/input/databang/filtered_metadata (2).csv\")\n\n# Fit trên toàn bộ user_id từ train + test\nuser_enc = LabelEncoder()\nuser_enc.fit(pd.concat([train_df[\"user_id\"], test_df[\"user_id\"]], ignore_index=True))\ntrain_df[\"uid\"] = user_enc.transform(train_df[\"user_id\"])\ntest_df[\"uid\"]  = user_enc.transform(test_df[\"user_id\"])\n\n# Fit item_id từ train + test + metadata\nitem_enc = LabelEncoder()\nitem_enc.fit(pd.concat([train_df[\"item_id\"], test_df[\"item_id\"], meta_df[\"item_id\"]], ignore_index=True))\ntrain_df[\"iid\"] = item_enc.transform(train_df[\"item_id\"])\ntest_df[\"iid\"]  = item_enc.transform(test_df[\"item_id\"])\nmeta_df[\"iid\"]  = item_enc.transform(meta_df[\"item_id\"])\n\n# Lấy đúng số lượng class (không dùng .nunique())\nnum_users = len(user_enc.classes_)\nnum_items = len(item_enc.classes_)\n\nprint(f\"✅ Encoded: {num_users} users, {num_items} items\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T14:50:32.245223Z","iopub.execute_input":"2025-05-27T14:50:32.245527Z","iopub.status.idle":"2025-05-27T14:50:44.070899Z","shell.execute_reply.started":"2025-05-27T14:50:32.245504Z","shell.execute_reply":"2025-05-27T14:50:44.070116Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 4: Prepare item text for LLM (concat description, features, categories)\nmeta_df[\"text_input\"] = (\n    meta_df[\"description\"].fillna(\"\") + \" \" +\n    meta_df[\"features\"].fillna(\"\")    + \" \" +\n    meta_df[\"categories\"].fillna(\"\")\n)\n\n# Reindex metadata so that row i corresponds to iid = i\nmeta_df = meta_df.set_index(\"iid\").reindex(range(num_items)).fillna(\"\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T14:50:46.585158Z","iopub.execute_input":"2025-05-27T14:50:46.585858Z","iopub.status.idle":"2025-05-27T14:50:46.649188Z","shell.execute_reply.started":"2025-05-27T14:50:46.585831Z","shell.execute_reply":"2025-05-27T14:50:46.648650Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === Cell 4.5: Tạo embedding cho toàn bộ user từ review_text ===\n\n# B1: Gộp review text theo user_id (chỉ user có review)\nuser_texts_partial = (\n    train_df.groupby(\"user_id\")[\"text\"]\n    .apply(lambda x: \" \".join(x.dropna().astype(str)))\n    .reset_index()\n)\n\n# B2: Gắn uid theo LabelEncoder\nuser_texts_partial[\"uid\"] = user_enc.transform(user_texts_partial[\"user_id\"])\n\n# B3: Tạo mảng văn bản với số lượng = num_users, default = \"\"\nuser_text_array = [\"\"] * num_users\nuser_text_dict = dict(zip(user_texts_partial[\"uid\"], user_texts_partial[\"text\"]))\n\nfor uid in range(num_users):\n    user_text_array[uid] = user_text_dict.get(uid, \"\")  # fallback nếu không có review\n\n# B4: Encode bằng SentenceTransformer\nuser_llm_np = llm.encode(user_text_array, batch_size=64, show_progress_bar=True)\nuser_llm_emb = torch.tensor(user_llm_np, dtype=torch.float32).to(device)\n\n# B5: Kiểm tra shape\nassert user_llm_emb.shape == (num_users, cfg.llm_dim)\nprint(\"✅ user_llm_emb shape:\", user_llm_emb.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T14:52:39.161319Z","iopub.execute_input":"2025-05-27T14:52:39.161644Z","iopub.status.idle":"2025-05-27T14:58:29.727343Z","shell.execute_reply.started":"2025-05-27T14:52:39.161620Z","shell.execute_reply":"2025-05-27T14:58:29.726713Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === Cell 4.6: Lưu lại các embedding đã tính sau Cell 4.5 ===\n\nsave_dir = \"/kaggle/working\"  # Hoặc \"./\" nếu chạy local notebook\n\ntorch.save(item_llm_emb.cpu(), f\"{save_dir}/item_llm_emb.pt\")\ntorch.save(user_llm_emb.cpu(), f\"{save_dir}/user_llm_emb.pt\")\n\nprint(\"✅ Đã lưu: item_llm_emb.pt và user_llm_emb.pt\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T15:06:53.963261Z","iopub.execute_input":"2025-05-27T15:06:53.964101Z","iopub.status.idle":"2025-05-27T15:06:54.347514Z","shell.execute_reply.started":"2025-05-27T15:06:53.964074Z","shell.execute_reply":"2025-05-27T15:06:54.346626Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === Load lại các embedding đã lưu (để tránh tính lại mỗi lần) ===\n\nitem_llm_emb = torch.load(f\"{save_dir}/item_llm_emb.pt\").to(device)\nuser_llm_emb = torch.load(f\"{save_dir}/user_llm_emb.pt\").to(device)\n\nprint(\"✅ Đã load embedding vào RAM & đưa lên GPU\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T15:06:56.245802Z","iopub.execute_input":"2025-05-27T15:06:56.246481Z","iopub.status.idle":"2025-05-27T15:06:56.384645Z","shell.execute_reply.started":"2025-05-27T15:06:56.246454Z","shell.execute_reply":"2025-05-27T15:06:56.384040Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n# Khởi tạo mô hình LLM (không truyền device để tránh lỗi CUDA)\nllm = SentenceTransformer(cfg.llm_model)\n\n# Chuẩn bị input văn bản cho từng item\nitem_texts = meta_df[\"text_input\"].tolist()  # length == num_items\n\n# Encode bằng LLM (trả về numpy), dùng GPU mặc định nếu có\nitem_emb_np = llm.encode(\n    item_texts,\n    batch_size=64,\n    show_progress_bar=True,\n    convert_to_numpy=True  # trả kết quả là numpy array\n)\n\n# Đảm bảo đúng shape đầu ra\nassert item_emb_np.shape == (num_items, cfg.llm_dim)\n\n# Chuyển sang torch tensor & lên GPU (nếu dùng)\nitem_llm_emb = torch.tensor(item_emb_np, dtype=torch.float32).to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T14:50:50.433392Z","iopub.execute_input":"2025-05-27T14:50:50.434229Z","iopub.status.idle":"2025-05-27T14:51:25.167065Z","shell.execute_reply.started":"2025-05-27T14:50:50.434194Z","shell.execute_reply":"2025-05-27T14:51:25.166289Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === Cell 6: Dataset explicit rating (thêm user embedding) ===\nclass RatingDataset(Dataset):\n    def __init__(self, df, item_emb, user_emb):\n        self.uids = df[\"uid\"].values\n        self.iids = df[\"iid\"].values\n        self.ratings = df[\"rating\"].values.astype(np.float32)\n        self.item_emb = item_emb.cpu()\n        self.user_emb = user_emb.cpu()\n\n    def __len__(self): return len(self.uids)\n\n    def __getitem__(self, idx):\n        u = torch.LongTensor([self.uids[idx]])\n        i = torch.LongTensor([self.iids[idx]])\n        r = torch.FloatTensor([self.ratings[idx]])\n        ll_item = self.item_emb[self.iids[idx]].unsqueeze(0)\n        ll_user = self.user_emb[self.uids[idx]].unsqueeze(0)\n        return u, i, ll_user, ll_item, r\n\n# Khởi tạo loader như thường lệ\ntrain_ds = RatingDataset(train_df, item_llm_emb, user_llm_emb)\ntest_ds  = RatingDataset(test_df,  item_llm_emb, user_llm_emb)\n\ntrain_loader = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True, num_workers=0)\ntest_loader  = DataLoader(test_ds,  batch_size=cfg.batch_size, shuffle=False, num_workers=0)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T15:07:02.515357Z","iopub.execute_input":"2025-05-27T15:07:02.515944Z","iopub.status.idle":"2025-05-27T15:07:02.760859Z","shell.execute_reply.started":"2025-05-27T15:07:02.515920Z","shell.execute_reply":"2025-05-27T15:07:02.760296Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === Cell 7: HybridNeuMF mở rộng với ll_user + ll_item ===\nclass HybridNeuMF(nn.Module):\n    def __init__(self, n_users, n_items, mf_dim, mlp_layers, llm_dim):\n        super().__init__()\n        self.user_mf = nn.Embedding(n_users, mf_dim)\n        self.item_mf = nn.Embedding(n_items, mf_dim)\n        self.user_mlp = nn.Embedding(n_users, mlp_layers[0] // 2)\n        self.item_mlp = nn.Embedding(n_items, mlp_layers[0] // 2)\n\n        blocks = []\n        for d_in, d_out in zip(mlp_layers[:-1], mlp_layers[1:]):\n            blocks += [nn.Dropout(0.2), nn.Linear(d_in, d_out), nn.ReLU()]\n        self.mlp = nn.Sequential(*blocks)\n\n        fusion_dim = mf_dim + mlp_layers[-1] + 2 * llm_dim\n        self.predict = nn.Sequential(\n            nn.Linear(fusion_dim, 64),\n            nn.ReLU(),\n            nn.Linear(64, 1)\n        )\n\n    def forward(self, u, i, ll_user, ll_item):\n        mu = self.user_mf(u).squeeze(1)\n        mi = self.item_mf(i).squeeze(1)\n        cf_vec = mu * mi\n\n        xu = self.user_mlp(u).squeeze(1)\n        xi = self.item_mlp(i).squeeze(1)\n        mlp_vec = self.mlp(torch.cat([xu, xi], dim=1))\n\n        x = torch.cat([cf_vec, mlp_vec, ll_user.squeeze(1), ll_item.squeeze(1)], dim=1)\n        return self.predict(x)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T15:07:04.731426Z","iopub.execute_input":"2025-05-27T15:07:04.731962Z","iopub.status.idle":"2025-05-27T15:07:04.739887Z","shell.execute_reply.started":"2025-05-27T15:07:04.731938Z","shell.execute_reply":"2025-05-27T15:07:04.739153Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === Cell 7.5: Khởi tạo model, optimizer (có regularization), loss ===\n\nmodel = HybridNeuMF(\n    n_users=num_users,\n    n_items=num_items,\n    mf_dim=cfg.mf_dim,\n    mlp_layers=cfg.mlp_layers,\n    llm_dim=cfg.llm_dim\n).to(device)\n\n# ✅ Thêm regularization: weight_decay > 0\noptimizer = torch.optim.Adam(\n    model.parameters(),\n    lr=cfg.lr,\n    weight_decay=cfg.weight_decay  # ví dụ: 1e-4 để L2 regularization\n)\n\ncriterion = nn.MSELoss()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T15:07:08.602970Z","iopub.execute_input":"2025-05-27T15:07:08.603238Z","iopub.status.idle":"2025-05-27T15:07:08.677148Z","shell.execute_reply.started":"2025-05-27T15:07:08.603217Z","shell.execute_reply":"2025-05-27T15:07:08.676636Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 8: Metrics (Precision/Recall/NDCG/MAP/MRR + RMSE real)\nimport numpy as np\n\ndef precision_at_k(ranked, truth, k):\n    return len(set(ranked[:k]) & set(truth))/k\n\ndef recall_at_k(ranked, truth, k):\n    return len(set(ranked[:k]) & set(truth))/len(truth) if len(truth)>0 else 0.0\n\ndef ndcg_at_k(ranked, truth, k):\n    dcg = sum(1/math.log2(idx+2) for idx,it in enumerate(ranked[:k]) if it in truth)\n    idcg= sum(1/math.log2(i+2) for i in range(min(len(truth),k)))\n    return dcg/idcg if idcg>0 else 0.0\n\ndef map_at_k(ranked, truth, k):\n    hits,s=0,0.0\n    for idx,it in enumerate(ranked[:k]):\n        if it in truth:\n            hits+=1; s+=hits/(idx+1)\n    return s/len(truth) if len(truth)>0 else 0.0\n\ndef mrr_at_k(ranked, truth, k):\n    for idx,it in enumerate(ranked[:k]):\n        if it in truth: return 1/(idx+1)\n    return 0.0\n\ndef rmse_real(model, loader):\n    model.eval()\n    se, n = 0.0, 0\n    with torch.no_grad():\n        for u, i, ll_user, ll_item, r in loader:\n            u, i = u.to(device), i.to(device)\n            ll_user, ll_item, r = ll_user.to(device), ll_item.to(device), r.to(device)\n\n            pred = model(u, i, ll_user, ll_item).view(-1)\n            se += ((pred - r.view(-1))**2).sum().item()\n            n  += r.numel()\n    return math.sqrt(se / n)\n\n@torch.no_grad()\ndef evaluate_full(model, train_df, test_df, K, loader):\n    model.eval()\n    train_map = train_df.groupby(\"uid\")[\"iid\"].apply(set).to_dict()\n    test_map  = test_df.groupby(\"uid\")[\"iid\"].apply(list).to_dict()\n\n    P, R, N, AP, MRR = [], [], [], [], []\n\n    for u, truth in test_map.items():\n        if not truth:\n            continue\n\n        users = torch.LongTensor([u] * num_items).to(device)\n        items = torch.arange(num_items).to(device)\n        ll_item = item_llm_emb.to(device)\n        ll_user = user_llm_emb[u].unsqueeze(0).repeat(num_items, 1).to(device)\n\n        scores = model(users, items, ll_user.unsqueeze(1), ll_item.unsqueeze(1)).view(-1).cpu().numpy()\n\n        for it in train_map.get(u, []):\n            scores[it] = -np.inf\n\n        ranked = np.argsort(-scores)\n        P.append(precision_at_k(ranked, truth, K))\n        R.append(recall_at_k(ranked, truth, K))\n        N.append(ndcg_at_k(ranked, truth, K))\n        AP.append(map_at_k(ranked, truth, K))\n        MRR.append(mrr_at_k(ranked, truth, K))\n\n    return {\n        \"Precision@K\": np.mean(P),\n        \"Recall@K\":    np.mean(R),\n        \"NDCG@K\":      np.mean(N),\n        \"MAP@K\":       np.mean(AP),\n        \"MRR@K\":       np.mean(MRR),\n        \"RMSE\":        rmse_real(model, loader)\n    }\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T15:07:10.734133Z","iopub.execute_input":"2025-05-27T15:07:10.734715Z","iopub.status.idle":"2025-05-27T15:07:10.747995Z","shell.execute_reply.started":"2025-05-27T15:07:10.734685Z","shell.execute_reply":"2025-05-27T15:07:10.747307Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"max uid in train_df =\", train_df[\"uid\"].max())\nprint(\"num_users in model  =\", model.user_mlp.num_embeddings)\n\nprint(\"max iid in train_df =\", train_df[\"iid\"].max())\nprint(\"num_items in model  =\", model.item_mlp.num_embeddings)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T15:07:14.383279Z","iopub.execute_input":"2025-05-27T15:07:14.384050Z","iopub.status.idle":"2025-05-27T15:07:14.396110Z","shell.execute_reply.started":"2025-05-27T15:07:14.384016Z","shell.execute_reply":"2025-05-27T15:07:14.395257Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === Cell 9: Training loop sử dụng ll_user + ll_item ===\nresults = []\nfor ep in range(1, cfg.epochs + 1):\n    model.train()\n    total_loss = 0.0\n\n    for u, i, ll_user, ll_item, r in tqdm(train_loader, desc=f\"Epoch {ep}\"):\n        u, i = u.to(device), i.to(device)\n        ll_user, ll_item, r = ll_user.to(device), ll_item.to(device), r.to(device)\n\n        optimizer.zero_grad()\n        pred = model(u, i, ll_user, ll_item).view(-1)\n        loss = criterion(pred, r.view(-1))\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item() * r.size(0)\n\n    train_loss = total_loss / len(train_ds)\n    metrics = evaluate_full(model, train_df, test_df, cfg.K, test_loader)\n    metrics[\"Train Loss\"] = train_loss\n    metrics[\"epoch\"] = ep\n\n    wandb.log(metrics)\n    results.append(metrics)\n\n    print(f\"Epoch {ep} — \" + \"  \".join(f\"{k}={v:.4f}\" for k, v in metrics.items() if k != \"epoch\"))\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T09:57:26.982229Z","iopub.execute_input":"2025-05-27T09:57:26.982509Z","iopub.status.idle":"2025-05-27T10:13:46.970044Z","shell.execute_reply.started":"2025-05-27T09:57:26.982488Z","shell.execute_reply":"2025-05-27T10:13:46.969273Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 10: Show & save\ndf_res = pd.DataFrame(results).set_index(\"epoch\")\ndisplay(df_res)\ntorch.save(model.state_dict(), \"/kaggle/working/hybrid_neumf_llm_newds.pth\")\ndf_res.to_csv(\"/kaggle/working/hybrid_neumf_llm_newds_metrics.csv\")\nprint(\"✅ Done.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T04:54:41.317785Z","iopub.execute_input":"2025-05-27T04:54:41.318065Z","iopub.status.idle":"2025-05-27T04:54:41.383038Z","shell.execute_reply.started":"2025-05-27T04:54:41.318043Z","shell.execute_reply":"2025-05-27T04:54:41.382353Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === Cell 6-N: Dataset implicit + negative sampling ===\nclass ImplicitDataset(Dataset):\n    def __init__(self, df_pos, num_items, item_emb, user_emb, num_neg=4):\n        self.pos = df_pos[[\"uid\", \"iid\"]].values\n        self.user_pos_dict = df_pos.groupby(\"uid\")[\"iid\"].apply(set).to_dict()\n        self.num_items = num_items\n        self.num_neg = num_neg\n        self.item_emb = item_emb.to(device)\n        self.user_emb = user_emb.to(device)\n\n    def __len__(self): return len(self.pos)\n\n    def __getitem__(self, idx):\n        u, i_pos = self.pos[idx]\n        triplets = [(u, i_pos, 1.0)]\n        neg = 0\n        while neg < self.num_neg:\n            i_neg = np.random.randint(0, self.num_items)\n            if i_neg not in self.user_pos_dict.get(u, set()):\n                triplets.append((u, i_neg, 0.0))\n                neg += 1\n        out = []\n        for (uu, ii, label) in triplets:\n            out.append((\n                torch.tensor([uu]), torch.tensor([ii]),\n                self.user_emb[uu].unsqueeze(0),\n                self.item_emb[ii].unsqueeze(0),\n                torch.tensor([label], dtype=torch.float32)\n            ))\n        return out\n\ndef collate_triplets(batch):\n    u, i, lu, li, l = zip(*[t for sub in batch for t in sub])\n    return torch.cat(u), torch.cat(i), torch.cat(lu), torch.cat(li), torch.cat(l)\n\ntrain_ds = ImplicitDataset(train_df, num_items, item_llm_emb, user_llm_emb, num_neg=4)\ntrain_loader = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True,\n                          collate_fn=collate_triplets, num_workers=0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T15:07:20.744616Z","iopub.execute_input":"2025-05-27T15:07:20.745042Z","iopub.status.idle":"2025-05-27T15:07:22.185705Z","shell.execute_reply.started":"2025-05-27T15:07:20.745015Z","shell.execute_reply":"2025-05-27T15:07:22.185039Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === Cell 8: Metric (precision@K, recall@K, ndcg@K...) ===\ndef precision_at_k(ranked, truth, k): return len(set(ranked[:k]) & set(truth)) / k\n\ndef recall_at_k(ranked, truth, k): return len(set(ranked[:k]) & set(truth)) / len(truth) if truth else 0.0\n\ndef ndcg_at_k(ranked, truth, k):\n    dcg = sum(1 / math.log2(idx + 2) for idx, it in enumerate(ranked[:k]) if it in truth)\n    idcg = sum(1 / math.log2(i + 2) for i in range(min(len(truth), k)))\n    return dcg / idcg if idcg > 0 else 0.0\n\ndef map_at_k(ranked, truth, k):\n    hits, s = 0, 0.0\n    for idx, it in enumerate(ranked[:k]):\n        if it in truth:\n            hits += 1\n            s += hits / (idx + 1)\n    return s / len(truth) if truth else 0.0\n\ndef mrr_at_k(ranked, truth, k):\n    for idx, it in enumerate(ranked[:k]):\n        if it in truth: return 1 / (idx + 1)\n    return 0.0\n\n@torch.no_grad()\ndef evaluate_full(model, train_df, test_df, K, loader):\n    model.eval()\n    train_map = train_df.groupby(\"uid\")[\"iid\"].apply(set).to_dict()\n    test_map = test_df.groupby(\"uid\")[\"iid\"].apply(list).to_dict()\n    P, R, N, AP, MRR = [], [], [], [], []\n    for u, truth in test_map.items():\n        if not truth: continue\n        users = torch.LongTensor([u] * num_items).to(device)\n        items = torch.arange(num_items).to(device)\n        ll_item = item_llm_emb.to(device)\n        ll_user = user_llm_emb[u].unsqueeze(0).repeat(num_items, 1).to(device)\n        scores = model(users, items, ll_user.unsqueeze(1), ll_item.unsqueeze(1)).view(-1).cpu().numpy()\n        for it in train_map.get(u, []):\n            scores[it] = -np.inf\n        ranked = np.argsort(-scores)\n        P.append(precision_at_k(ranked, truth, K))\n        R.append(recall_at_k(ranked, truth, K))\n        N.append(ndcg_at_k(ranked, truth, K))\n        AP.append(map_at_k(ranked, truth, K))\n        MRR.append(mrr_at_k(ranked, truth, K))\n    return {\n        \"Precision@K\": np.mean(P),\n        \"Recall@K\": np.mean(R),\n        \"NDCG@K\": np.mean(N),\n        \"MAP@K\": np.mean(AP),\n        \"MRR@K\": np.mean(MRR),\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T15:07:25.655741Z","iopub.execute_input":"2025-05-27T15:07:25.656005Z","iopub.status.idle":"2025-05-27T15:07:25.671435Z","shell.execute_reply.started":"2025-05-27T15:07:25.655985Z","shell.execute_reply":"2025-05-27T15:07:25.670716Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"@torch.no_grad()\ndef compute_rmse(model, data_loader):\n    model.eval()\n    all_preds, all_labels = [], []\n    for u, i, ll_user, ll_item, r in data_loader:\n        u, i, ll_user, ll_item, r = u.to(device), i.to(device), ll_user.to(device), ll_item.to(device), r.to(device)\n        logits = model(u, i, ll_user, ll_item).view(-1)\n        probs = torch.sigmoid(logits)\n        all_preds.append(probs.cpu())\n        all_labels.append(r.cpu())\n    pred = torch.cat(all_preds)\n    true = torch.cat(all_labels)\n    rmse = torch.sqrt(torch.mean((pred - true) ** 2)).item()\n    return rmse\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T15:07:30.233789Z","iopub.execute_input":"2025-05-27T15:07:30.234050Z","iopub.status.idle":"2025-05-27T15:07:30.240270Z","shell.execute_reply.started":"2025-05-27T15:07:30.234031Z","shell.execute_reply":"2025-05-27T15:07:30.239539Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === Cell 9: Training loop with early stopping ===\nbest_ndcg, patience, counter = 0.0, 3, 0\nresults = []\n\nfor ep in range(1, cfg.epochs + 1):\n    model.train()\n    total_loss = 0.0\n\n    for u, i, ll_user, ll_item, r in tqdm(train_loader, desc=f\"Epoch {ep}\"):\n        u, i, ll_user, ll_item, r = u.to(device), i.to(device), ll_user.to(device), ll_item.to(device), r.to(device)\n        optimizer.zero_grad()\n        pred = model(u, i, ll_user, ll_item).view(-1)\n        loss = criterion(pred, r)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item() * r.size(0)\n\n    train_loss = total_loss / len(train_loader.dataset)\n    metrics = evaluate_full(model, train_df, test_df, cfg.K, None)\n    rmse = compute_rmse(model, train_loader)\n    metrics[\"RMSE\"] = rmse\n\n    metrics[\"Train Loss\"] = train_loss\n    metrics[\"epoch\"] = ep\n    wandb.log(metrics)\n    results.append(metrics)\n\n    print(f\"Epoch {ep} — \" + \"  \".join(f\"{k}={v:.4f}\" for k, v in metrics.items() if k != \"epoch\"))\n\n    ndcg = metrics[\"NDCG@K\"]\n    if ndcg > best_ndcg:\n        best_ndcg = ndcg\n        counter = 0\n        torch.save(model.state_dict(), \"best_model.pt\")\n    else:\n        counter += 1\n        if counter >= patience:\n            print(f\"\\n🛑 Early stopping at epoch {ep} — best NDCG@K = {best_ndcg:.4f}\")\n            break\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T15:07:34.289411Z","iopub.execute_input":"2025-05-27T15:07:34.289690Z","iopub.status.idle":"2025-05-27T16:00:01.495660Z","shell.execute_reply.started":"2025-05-27T15:07:34.289668Z","shell.execute_reply":"2025-05-27T16:00:01.494884Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === Compute RMSE on train and test sets (fixed version) ===\n@torch.no_grad()\ndef compute_rmse(model, data_loader):\n    model.eval()\n    all_preds, all_labels = [], []\n    for u, i, ll_user, ll_item, r in data_loader:\n        u, i, ll_user, ll_item, r = u.to(device), i.to(device), ll_user.to(device), ll_item.to(device), r.to(device)\n        logits = model(u, i, ll_user, ll_item).view(-1)\n        probs = torch.sigmoid(logits)\n        all_preds.append(probs.cpu())\n        all_labels.append(r.cpu())\n    pred = torch.cat(all_preds)\n    true = torch.cat(all_labels)\n    rmse = torch.sqrt(torch.mean((pred - true) ** 2)).item()\n    return rmse\n\n# Dataset cho test (dành cho explicit feedback)\nclass ExplicitDataset(Dataset):\n    def __init__(self, df, user_emb, item_emb):\n        self.pairs = df[[\"uid\", \"iid\"]].values\n        self.labels = np.ones(len(df))  # vì chỉ có positive interactions\n        self.user_emb = user_emb.to(device)\n        self.item_emb = item_emb.to(device)\n\n    def __len__(self): return len(self.pairs)\n\n    def __getitem__(self, idx):\n        u, i = self.pairs[idx]\n        return (\n            torch.tensor([u]), torch.tensor([i]),\n            self.user_emb[u].unsqueeze(0),\n            self.item_emb[i].unsqueeze(0),\n            torch.tensor([self.labels[idx]], dtype=torch.float32)\n        )\n\n# Collate function riêng cho explicit dataset\ndef collate_explicit(batch):\n    u, i, lu, li, l = zip(*batch)\n    return torch.cat(u), torch.cat(i), torch.cat(lu), torch.cat(li), torch.cat(l)\n\n# Load best model từ file\nmodel.load_state_dict(torch.load(\"best_model.pt\"))\nmodel.eval()\n\n# Tạo DataLoader cho test\ntest_ds = ExplicitDataset(test_df, user_llm_emb, item_llm_emb)\ntest_loader = DataLoader(test_ds, batch_size=cfg.batch_size, shuffle=False, collate_fn=collate_explicit)\n\n# Tính RMSE cho train và test\nrmse_train = compute_rmse(model, train_loader)\nrmse_test  = compute_rmse(model, test_loader)\n\n# In kết quả\nprint(f\"✅ RMSE on train set: {rmse_train:.4f}\")\nprint(f\"✅ RMSE on test set : {rmse_test:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T11:22:43.765767Z","iopub.execute_input":"2025-05-27T11:22:43.766476Z","iopub.status.idle":"2025-05-27T11:24:43.926018Z","shell.execute_reply.started":"2025-05-27T11:22:43.766452Z","shell.execute_reply":"2025-05-27T11:24:43.925161Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\n@torch.no_grad()\ndef compute_hr_at_k(model, train_df, test_df, item_llm_emb, user_llm_emb, K=10):\n    model.eval()\n    num_items = item_llm_emb.shape[0]\n    train_map = train_df.groupby(\"uid\")[\"iid\"].apply(set).to_dict()\n    test_map = test_df.groupby(\"uid\")[\"iid\"].apply(list).to_dict()\n    HR = []\n\n    for u, truth in test_map.items():\n        if not truth:\n            continue\n\n        users = torch.LongTensor([u] * num_items).to(device)\n        items = torch.arange(num_items).to(device)\n        ll_item = item_llm_emb.to(device)\n        ll_user = user_llm_emb[u].unsqueeze(0).repeat(num_items, 1).to(device)\n        scores = model(users, items, ll_user.unsqueeze(1), ll_item.unsqueeze(1)).view(-1).cpu().numpy()\n\n        for it in train_map.get(u, []):  # exclude training items\n            scores[it] = -np.inf\n\n        ranked = np.argsort(-scores)[:K]\n        hit = any(i in ranked for i in truth)\n        HR.append(int(hit))\n\n    return np.mean(HR)\n\n# Gọi hàm này sau khi load mô hình tốt nhất\nmodel.load_state_dict(torch.load(\"best_model.pt\"))\nmodel.eval()\n\nhr10 = compute_hr_at_k(model, train_df, test_df, item_llm_emb, user_llm_emb, K=10)\nprint(f\"✅ HR@10: {hr10:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T16:01:07.083575Z","iopub.execute_input":"2025-05-27T16:01:07.083895Z","iopub.status.idle":"2025-05-27T16:02:25.759161Z","shell.execute_reply.started":"2025-05-27T16:01:07.083875Z","shell.execute_reply":"2025-05-27T16:02:25.758423Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}